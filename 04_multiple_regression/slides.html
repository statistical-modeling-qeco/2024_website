<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Multiple linear regression and model selection</title>
    <meta charset="utf-8" />
    <meta name="author" content="PI (Paulo Inácio)" />
    <meta name="date" content="2024-01-31" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <script src="libs/clipboard/clipboard.min.js"></script>
    <link href="libs/shareon/shareon.min.css" rel="stylesheet" />
    <script src="libs/shareon/shareon.min.js"></script>
    <link href="libs/xaringanExtra-shareagain/shareagain.css" rel="stylesheet" />
    <script src="libs/xaringanExtra-shareagain/shareagain.js"></script>
    <script src="libs/xaringanExtra_fit-screen/fit-screen.js"></script>
    <link href="libs/tachyons/tachyons.min.css" rel="stylesheet" />
    <link href="libs/tile-view/tile-view.css" rel="stylesheet" />
    <script src="libs/tile-view/tile-view.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Multiple linear regression and model selection
]
.subtitle[
## Quantitative Ecology Training Program - Serrapilheira
]
.author[
### PI (Paulo Inácio)
]
.date[
### 31 Jan 2024
]

---







## Key concepts

* Multiple linear regression

* Linear predictor

* Additive predictor

* Predictor with interactions

* Over-fitting and under-fitting

* AIC (for Gaussian models)

* Model selection

---

## Multiple linear regression (Gaussian)

We can extend the simple linear regression model to accommodate
multiple predictors. These are the **multiple linear regression**
models, of which the simple linear regression is just a particular
case with a single predictor variable:

`$$\begin{align}
y_i &amp;\sim \text{Normal}(\mu_i, \sigma) \\
\mu_i &amp; = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_j x_{ij} \\
\sigma &amp; = C
\end{align}$$`

---

## A quick example to warm up

* **Question**: what's the impact of herbivory on plant fitness?
* Field experiment: 40 plants of *Ipomopsis agreggata* (Pursh)
V.E.Grant assigned at random to two treatments: unprotected from
grazing by rabbits and protect from grazing by fenced cages.
* **Response variable**: fruit yield of each plant (mg dry mass)
* **Predictor variables**: 
  * Treatment:  `\(0 =\)` control (non-fenced), `\(1 =\)` fenced
  * Basal diameter of each plant (mm)
* Data from: Crawley (2007) The R Book.

.center[

&lt;img src="figs/ipomopsis_and_grazing_cage.png" width="50%" /&gt;

]


---

## The data

.pull-left[

```
##   fruit.w treatment diameter
## 1   59.77    Fenced    6.225
## 2   60.98    Fenced    6.487
## 3   14.73    Fenced    4.919
## 4   19.28    Fenced    5.130
## 5   34.25    Fenced    5.417
## 6   35.53    Fenced    5.359
```
...


```
##    fruit.w treatment diameter
## 35   98.47   Control    9.351
## 36   40.15   Control    7.066
## 37   52.26   Control    8.158
## 38   46.64   Control    7.382
## 39   71.01   Control    8.515
## 40   83.03   Control    8.530
```
]

.pull-right[
.center[
![](slides_files/figure-html/data-1.png)&lt;!-- --&gt;
]]

---

## Fitting multiple regression in R

### The model

`$$\begin{align}
\text{Yield}_i &amp;\sim \text{Normal}(\mu_i, \sigma) \\
\mu_i &amp; = \beta_0 + \beta_1 \text{diameter}_{i} + \beta_2 \text{treatment}_{i} \\
\sigma &amp; = C
\end{align}$$`



### Call in R


```r
ipo_m1 &lt;- lm(fruit.w ~ diameter + treatment,
             data = ipomopsis)
```


---

## Coefficients of the fitted model



```r
summary( ipo_m1 ) 
```

```
## 
## Call:
## lm(formula = fruit.w ~ diameter + treatment, data = ipomopsis)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -17.1920  -2.8224   0.3223   3.9144  17.3290 
## 
## Coefficients:
##                 Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)     -127.829      9.664  -13.23 1.35e-15 ***
## diameter          23.560      1.149   20.51  &lt; 2e-16 ***
## treatmentFenced   36.103      3.357   10.75 6.11e-13 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 6.747 on 37 degrees of freedom
## Multiple R-squared:  0.9291,	Adjusted R-squared:  0.9252 
## F-statistic: 242.3 on 2 and 37 DF,  p-value: &lt; 2.2e-16
```




---

## Coefficients of the fitted model

`$$\begin{align}
\mu_i &amp; = -128 + 24 \times \text{diameter}_{i} + 36 \times \text{treatment}_{i}
\end{align}$$`

### For treatment = 0 (control plants):

`$$\begin{align}
\mu_i &amp; = -128 + 24 \times \text{diameter}_{i} + 36 \times 0 \\
      &amp; = -128 + 24 \times \text{diameter}_{i}
\end{align}$$`

--

### For treatment = 1 (fenced plants):

`$$\begin{align}
	\mu_i &amp; = -128 + 24 \times \text{diameter}_{i} + 36 \times 1 \\
	&amp; = (36 -128) + 24 \times \text{diameter}_{i}
\end{align}$$`

---

## Plot with observed and predicted values

.pull-left[

* The expected value of fruit biomass increases with plant size

* For any plant size, caging increased fruit yield in 36 mg, on average.
]

.pull-right[
.center[

![](slides_files/figure-html/unnamed-chunk-6-1.png)&lt;!-- --&gt;
]]

---

## The linear predictor

The expression for the predicted value in multiple linear regression can be written in the compact form:

`$$\mu_i = \sum_{j=0}^{J} \beta_{j} x_{ij} \, = \, \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_j x_{ij}$$` 

Where `\(J\)` is the number of predictor variables. This is called the **linear predictor**.

---

## Linearity in the parameters

For statistical models, a predictor is linear if it is a linear
function of parameters, given an observed value of the predictor variables.

--

Is the predictor `\(\mu_i = \beta_0 + \beta_1 x_{i}^2\)` linear in the parameters?

--

**Yes**! For a given observed value of the predictor variable `\(x_{i}\)`,
say `\(x_{i} = 2\)`, the predictor is 

`$$\mu_i = 2 \beta_0 + 4 \beta_1$$`

Which is a linear expression.

---

## Some linear predictors

In summary: linear predictors are any weighted sum of
predictor terms. The weights are the linear coefficients. 

--

Each term in this weighted sum can be any mathematical function of predictor
variables, provided this function does not include the coefficients of
the models. Some examples:

--

`$$\begin{align}
\mu_i &amp; = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} +  \beta_3 x_{i3} \\
\mu_i &amp; = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i1}^2 \\
\mu_i &amp; = \beta_0 + \beta_1 \log(x_{i1}) + \beta_2 \sqrt(x_{i2}) \\
\mu_i &amp; = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} +  \beta_3 x_{i1}x_{i2}
\end{align}$$`

---

## Non-linear predictors

For statistical models, non-linear predictors result in
non-linear functions of one of more parameters, given the observation of the predictor variables. 

--

For example, the power function `\(\mu_i = \beta_0 x^_{i}{\beta_1}\)` is a non-linear predictor.

--

To see that, take some value for `\(x_{i}\)`, say `\(x_{i}=2\)`. The power function will be 

`$$\mu_i = \beta_0 \times 2^{\beta_1}$$` 

which is not a linear function of the parameters.

---

## Interactions

We have seen that the linear predictor can include products of predictor variables, like:

`$$\begin{align}
y_i &amp;\sim \text{Normal}(\mu_i, \sigma) \\
\mu_i &amp; = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} +  \beta_3 x_{i1} x_{i2}\\
\sigma &amp; = C
\end{align}$$`

These products, as the term `\(x_{i1} x_{i2}\)` are called **interactions**.

When two predictor variables interact, their effects are no longer additive.

Rather, the effect of one interacting variable depends on the value of
the other interacting variable (think about the interaction between two drugs).

---

## A simple example of interactions

* **Question**: what are the best soil moisture and light availability to grow tulips?
* Greenhouse experiment: beds of tulips kept in nine combinations of
  soil moisture and shading. Three replicates for each combination
  (total of 27 beds).
* **Response variable**: mean plant height in each bed
* **Predictor variables**: 
  * Watering: `\(1 =\)` low, `\(2 =\)` medium, `\(3 =\)` high
  * Shading: `\(1 =\)` low, `\(2 =\)` medium, `\(3 =\)` high 
* Data from: Grafen &amp; Hails (2002) Modern Statistics for the Life Sciences.

.center[
![](figs\tulips.jpg)
]

---

## The data

.pull-left[


```
##   bed water shade blooms
## 1   a     1     1   0.00
## 2   a     1     2   0.00
## 3   a     1     3 111.04
## 4   a     2     1 183.47
## 5   a     2     2  59.16
## 6   a     2     3  76.75
```


```
##    bed water shade blooms
## 22   c     2     1 246.00
## 23   c     2     2 135.92
## 24   c     2     3  90.66
## 25   c     3     1 304.52
## 26   c     3     2 249.33
## 27   c     3     3 134.59
```
]

.pull-right[
.center[

![](slides_files/figure-html/unnamed-chunk-9-1.png)&lt;!-- --&gt;

]]

---

## Fitting a model with interaction in R

### The model

`$$\begin{align}
\text{Mean height}_i &amp;\sim \text{Normal}(\mu_i, \sigma) \\
\mu_i &amp; = \beta_0 + \beta_1 \text{watering}_{i} + \beta_2 \text{shading}_{i} + \beta_3 \text{shading}_{i} \text{watering}_{i}\\
\sigma &amp; = C
\end{align}$$`


### Fitting in R


```r
tulip_int &lt;- lm(blooms ~ water + shade + water:shade,
                data = tulips)
```


---

## Coefficients of the fitted model



```r
summary(tulip_int) 
```

```
## 
## Call:
## lm(formula = blooms ~ water + shade + water:shade, data = tulips)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -121.03  -26.92    5.27   35.46   75.40 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  -150.81      66.03  -2.284  0.03192 *  
## water         181.50      30.56   5.939  4.7e-06 ***
## shade          64.10      30.56   2.097  0.04716 *  
## water:shade   -52.85      14.15  -3.736  0.00108 ** 
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 49.01 on 23 degrees of freedom
## Multiple R-squared:  0.7526,	Adjusted R-squared:  0.7204 
## F-statistic: 23.33 on 3 and 23 DF,  p-value: 3.663e-07
```

---

## Coefficients of the fitted model

`$$\begin{align}
\mu_i &amp; = -151 + 181 \times \text{water}_{i} + 64 \times \text{shade}_{i} -53 \times \text{shade}_{i} \times  \text{water}_{i}  \\
\end{align}$$`

--

###  For shade = 1 :

`$$\begin{align}
\mu_i &amp; = -151 + 181 \text{water}_{i} + 64 \times 1 + (-53 \times 1) \text{water}_{i}  \\
      &amp; = (64 -151)  + (181 - 53) \text{water}_{i}
\end{align}$$`

--

### For shade = 3:

`$$\begin{align}
\mu_i &amp; = -151 + 181 \text{water}_{i} + (64 \times 3) + ( - 53 \times 3) \text{water}_{i}  \\
      &amp; = (192 -151)  + (181 - 159) \text{water}_{i} 
\end{align}$$`

---

## Plot with observed and predicted values

.center[

![](slides_files/figure-html/unnamed-chunk-12-1.png)&lt;!-- --&gt;

]


---

## Inferences about competing models



.pull-left[

**The data-generating process (a.k.a. true model):**

`$$\begin{align}
y_i &amp;\sim \mathcal{N}(\mu_i, \sigma) \\
\mu_i &amp; = e^{(x_i - 0.3)^2} -1 \\
\sigma &amp; = C
\end{align}$$`
]

.pull-right[
.center[

![](slides_files/figure-html/unnamed-chunk-14-1.png)&lt;!-- --&gt;

]]

---

## The models

.pull-left[
$$
`\begin{align}
\textbf{M1}: &amp; \mu_i = \beta_0 + \beta_1 x_i \\[1em]
\textbf{M2}: &amp; \mu_i = \beta_0 + \beta_1 x_i + \beta_2 {x_i}^2\\[1em]
\textbf{M3}: &amp; \mu_i = \beta_0 + \beta_1 x_i + \beta_2 {x_i}^2 + \\
&amp;+ \beta_3 {x_i}^3  + \beta_4 {x_i}^4 + \beta_5 {x_i}^5
\end{align}`
$$
]

.pull-right[
.center[
![](slides_files/figure-html/unnamed-chunk-15-1.png)&lt;!-- --&gt;

]]

---

## Over-fitting

.pull-left[


```r
M3 &lt;- lm(y1 ~ x + I(x^2) + I(x^3) +
             I(x^4) + I(x^5), data = df1)
summary(M3)$coefficients[,1:2]
```

```
##                Estimate Std. Error
## (Intercept)   0.1288442  0.1242737
## x            -5.9906302  2.9794528
## I(x^2)       39.7181676 19.3754228
## I(x^3)      -96.3146701 50.0956063
## I(x^4)      100.1062449 56.0652514
## I(x^5)      -37.0336895 22.6046486
```

&gt; To make inferences about unique features of the data at hand, as if they applied to all (or most all) samples (hence the population).
&gt;
.right[Burham &amp; Anderson (2002).]
]

.pull-right[
.center[
![](slides_files/figure-html/unnamed-chunk-17-1.png)&lt;!-- --&gt;
]
]

---

## Under-fitting

.pull-left[


```r
M1 &lt;- lm(y1 ~ x,
         data = df1)

summary(M1)$coefficients[,1:2]
```

```
##              Estimate Std. Error
## (Intercept) -0.183588 0.06329131
## x            0.694772 0.09810677
```


&gt; Failure to identify features in the data-generating process that are
&gt; strongly replicable.
&gt;
.right[Burham &amp; Anderson (2002).]
]

.pull-right[
.center[

![](slides_files/figure-html/unnamed-chunk-19-1.png)&lt;!-- --&gt;
]
]

---

## A good fit


.pull-left[


```r
M2 &lt;- lm(y1 ~ x + I(x^2), data = df1)

summary(M2)$coefficients[,1:2]
```

```
##                Estimate Std. Error
## (Intercept) -0.01266738 0.07777414
## x           -0.32530671 0.34919767
## I(x^2)       0.97256083 0.32356492
```
]

.pull-right[
.center[
![](slides_files/figure-html/unnamed-chunk-21-1.png)&lt;!-- --&gt;
]
]


---


## Akaike information criterion

.left-column[
.center[
![](../05_maximum_likelihood/figs/Akaike.jpg)
Hirotugu Akaike (1997-2009)
]]

.right-column[


AIC estimates a statistical distance between two probability
distributions: the true one, which is the reference, and a given model
fitted to a sample from the true distribution.

AIC is an estimate of the information of the true data-generating 
distribution that is preserved by a model.

More specifically, AIC is an estimate of the Kullback-Leibler
divergence (or K-L relative entropy) of the model to the reference
true distribution that generated the data.  
]


---

## AIC for Gaussian models


`$$\textrm{AIC} \, = \, N \log (\widehat{\sigma^2}) + 2\,K$$`

Where `\(\widehat{\sigma^2}\)` is the estimate of the residual variance of
the model, that is, the mean sum of squares of residuals:

$$ \widehat{\sigma^2} \, = \, \frac{1}{N} \, \sum_{i = 1}^N (Y_i - \widehat{Y_i})^2 $$

And `\(K\)` is the number of parameters of the model.





---


## How to use AIC

* AIC expresses distance to the true model, or loss of information by the fitted model;

* Thus, the model with the lowest value of AIC among a set of
  competing models is the most plausible one (or best supported by the
  data);
  
* Canonical rule: models that differ `\(\leq 2\)` in their AIC values are
  equally supported by data;

* To ease model selection, we calculate `\(\Delta \mathrm{AIC}\)`: 
`$$\Delta_i \, = \, \textrm{AIC}_i - \min (\textrm{AIC})$$`

* The best supported , or more plausible, model will have `\(\Delta_i = 0\)`

---

## AICc: correction for small samples

For  `\(n/K \, &lt; \,40\)`, multiply `\(K\)` by the correction term


$$ \left(\frac{n}{n-K-1}\right) $$

### AICc for Gaussian models:

$$ \textrm{AICc} \, = \, -2\,\textbf{L}_y(\widehat{\theta}) + 2\,K \left(\frac{n}{n-K-1}\right) $$


Where `\(n\)` is the sample size.


---

## Evidence weights 

`$$w_i\,=\, \frac{e^{-1/2 \Delta_i}}{\sum e^{-1/2 \Delta_i}}$$`

* Evidence or Akaike weights sum up one;

* Thus, `\(w_i\)` express the relative support of each model in the set of
  competing models, in a standardized scale;

* In a frequentist approach, `\(w_i\)` estimates the probability that each
  model will be the best supported one, if you repeat the sample and then the selection many
  times.


---

## Model selection with AIC: example

.pull-left[



```r
library(bbmle)
M1 &lt;- lm(y1 ~ x, data = df1)
M2 &lt;- lm(y1 ~ x + I(x^2), data = df1)
M3 &lt;- lm(y1 ~ x + I(x^2) + I(x^3) +
             I(x^4) + I(x^5), data = df1)

AICctab(M1, M2, M3, logLik=TRUE, base = TRUE,
        weights = TRUE, nobs = nrow(df1))
```

```
##    logLik AICc  dLogLik dAICc df weight
## M2  16.7  -23.0   4.3     0.0 4  0.867 
## M3  20.4  -18.1   7.9     4.9 7  0.076 
## M1  12.5  -17.5   0.0     5.5 3  0.057
```

]


.pull-right[
.center[

![](slides_files/figure-html/unnamed-chunk-23-1.png)&lt;!-- --&gt;
]]


---

## Final remarks on AIC

   * Ties can and do happen: more than one model with `\(\Delta_i &lt; 2\)`
   tell us that the data does not contain enough evidence to spot
   the best model;
   
   * Model selection with AIC is not a statistical test; 
   
   * Model selection is restricted to competing models: if all
     competing models are bad, the selected model will just be the
     least bad.
   
   * AIC does not express goodness of fit. The selected model can still have a poor fit; 
   
   * AIC cannot be used to compare models fitted to different datasets; 
   
   * For the same reason, the AIC cannot be used to compare models
     fitted to transformed and untransformed data.
   
---



## Further reading

+ Burnham, K. P., &amp; Anderson, D. R. (2002). Model Selection and
  Multimodel Inference: A Practical-Theoretic Approach, 2nd ed. New
  York, Springer-Verlag.

+ Bolker, B.M. 2008 Ecological
  Models and Data in R. Princeton: Princeton University Press.

+ McElreath, R., 2018. Statistical Rethinking: A Bayesian Course with
  Examples in R and Stan, 2nd ed. CRC Press.
  
  
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
