---
title: "Introduction to regression and linear models"
subtitle: "Serrapilheira/ICTP-SAIFR Training Program in Quantitative Biology and Ecology - 2023"  
author: 
  - "Andrea SÃ¡nchez-Tapia"
date: 'January 12th-13th 2023'
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: xaringan-themer.css
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: '16:9'
self-contained: true
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
options(servr.daemon = TRUE)#para que no bloquee la sesiÃ³n
knitr::opts_chunk$set(echo = TRUE, message = F, warning = F)
```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_mono_accent(
  base_color = "#03045E",
  
  colors = c(
    red = "#A70001",
    white = "#FFFFFF",
    black = "#181818"
  ),
  text_bold_color = "#03045E",
  header_font_google = google_font("Roboto Condensed"),
  text_font_google = google_font("Roboto Condensed", "300", "300i"),
  code_font_google = google_font("Fira Mono"),
  text_font_size = "28px",
  code_font_size = "28px"
)
xaringanExtra::use_share_again()

# clipboard
htmltools::tagList(
  xaringanExtra::use_clipboard(
    button_text = "Copy code <i class=\"fa fa-clipboard\"></i>",
    success_text = "Copied! <i class=\"fa fa-check\" style=\"color: #90BE6D\"></i>",
    error_text = "Not copied ðŸ˜• <i class=\"fa fa-times-circle\" style=\"color: #F94144\"></i>"
  ),
  rmarkdown::html_dependency_font_awesome()
)

```

```{r generate_data, echo = F}

library(wesanderson)
library(dplyr)
library(ggplot2)

cor <- wes_palette("Rushmore1") #("FantasticFox1")


set.seed(4)
x1 <- seq(1, 5, by = 0.5)
mu <- 1.2 + 3.5 * x1
#res = rnorm(n = 9, mean = 0, sd = 5)
y1 <- rnorm(n = 9, mean = mu, sd = 3)
#y1 = y0 + res
xy <- data.frame(x1, mu, res = y1 - mu, y1)
#plot(y1~x1)
#abline(1.2, 3.5)
#abline(coef(lm(y1~x1)), col = "red")

## plot(x1, y1)
## abline(lm(y1~x1), col = "red")
## summary(lm(y1~x1))


xy$lmpred <- predict(lm(y1~x1))
xy$lmres <- residuals(lm(y1~x1))

prev <- xy$lmpred

```

## In statistical models

+ We deal with __predictor__ and __response__ variables. 

--

+ In experimental settings this distinction is clear, we __manipulate__ the predictor variables and __measure/observe__ the response variables.

--

+ In observational settings ("natural experiments"), we do not manipulate the predictor variables but sample the response variable accross them. 

--

+ We hypothesize the __causation relationship__ and build the model accordingly.



---
## Linear model

+ The simplest possible model is one that describes a single variable $X$ having a direct effect on the response variable $Y$. $X \rightarrow Y$

--

+ The __response variable__ has a distribution, linear regression assumes it has a __normal distribution__. 

--

+ The relationship between $X$ and $Y$ can be described by the equation of a line where $\beta_0$ is the __intercept__ and $\beta_1$ is the __slope__

--

$$
Y_i \sim \mathcal{N}(\mu_i, \sigma) \\
$$

$$
\mu_i = \beta_0 + \beta_1 X_i 
$$

---

## Assumptions of linear regression

+ The relationship is linear 

--

+ The residuals are normally distributed

--

+ Variance is constant for all $X$ values: __homoscedasticity__

---

## How do we estimate the intercept and slope?

.pull-left[


```{r dataset, echo = FALSE, fig.align='center'}
plot(y1 ~ x1, las = 1, bty = "l", cex = 3, col = cor[3],
     #xaxt="n", yaxt="n",
     xlab = "predictor", ylab = "response",
     pch = 19, cex.axis = 1.5, cex.lab = 1.5)
#points(x = mean(x1), y = mean(y1), pch = 19, cex = 1)
#points(x = mean(x1), y = mean(y1), pch = 1, cex = 2.2)
#text("fulcrum", x = mean(x1) + .5, y = mean(y1), cex = 1.6)
abline(a = 16.8 , b = -1, col = "grey50")
abline(a = 15 , b = -1.5, col = "grey50")
abline(a = -10 , b = 8.07, col = "grey50")
abline(a = 12 , b = 4, col = "grey50")
abline(lm(y1 ~ x1), col = cor[2])
```

]

.pull-right[

+ The simplest approach to do this: __Ordinary Least Squares__


]
```{r rethinking, echo = F, eval = F}
library(rethinking)
data(Howell1)
d <- Howell1
d2 <- d[ d$age >= 18 , ]
plot(d2$height~d2$weight)

# #set.seed(2971)
# N <- 100
# a <- rnorm( N , 178 , 20 ) 
# b <- rnorm( N , 0 , 10 )
# plot(NULL , xlim = range(d2$weight) , ylim=c(-100,400) , xlab="weight" , ylab="height" )
# abline( h=0 , lty=2 )
# abline( h=272 , lty=1 , lwd=0.5 )
# mtext( "b ~ dnorm(0,10)" )
# xbar <- mean(d2$weight)
# for (i in 1:N ) curve( a[i] + b[i]*(x - xbar) ,
# from=min(d2$weight) , to=max(d2$weight) , add=TRUE , col=col.alpha("black",0.2) )

```



---
## How do we estimate the intercept and slope?

.pull-left[


```{r ols2, echo = FALSE, fig.align='center'}
plot(y1 ~ x1, las = 1, bty = "l", cex = 3, col = cor[3],
     #xaxt="n", yaxt="n",
     xlab = "predictor", ylab = "response",
     pch = 19, cex.axis = 1.5, cex.lab = 1.5)
#points(x = mean(x1), y = mean(y1), pch = 19, cex = 1)
#points(x = mean(x1), y = mean(y1), pch = 1, cex = 2.2)
#text("fulcrum", x = mean(x1) + .5, y = mean(y1), cex = 1.6)
abline(a = 16.8 , b = -1, col = "grey50")
abline(a = 15 , b = -1.5, col = "grey50")
abline(a = -10 , b = 8.07, col = "grey50")
abline(a = 12 , b = 4, col = "grey50")
abline(lm(y1 ~ x1), col = cor[2])
```

]

.pull-right[

+ The simplest approach to do this: __Ordinary Least Squares__

+ For _any_ line, the observed values of $Y$ will be different from the predicted values $\hat{Y}$


]

---
## The residual sum of squares RSS

.pull-left[

```{r residuals, echo = FALSE, fig.align='center'}
plot(y1 ~ x1, las = 1, bty = "l", cex = 3, col = cor[3],
     #xaxt="n", yaxt="n",
     xlab = "predictor", ylab = "response",
     pch = 19, cex.axis = 1.5, cex.lab = 1.5)
segments(x0 = x1, x1 = x1, y0 = y1, y1 = prev, lty = 2, lwd = 2)
abline(lm(y1 ~ x1), lwd = 3, col = cor[2], cex = 2)
```
]


.pull-left[

+ __Residuals__ are the difference between $Y_i$ and $\hat{Y_i}$. 

+ The magnitude of the deviations is more important than their sign, so we square them $(Y_i -\hat{Y_i})^2$

 
]

---
## The residual sum of squares RSS

.pull-left[

```{r residuals2, echo = FALSE, fig.align='center'}
plot(y1 ~ x1, las = 1, bty = "l", cex = 3, col = cor[3],
     #xaxt="n", yaxt="n",
     xlab = "predictor", ylab = "response",
     pch = 19, cex.axis = 1.5, cex.lab = 1.5)
segments(x0 = x1, x1 = x1, y0 = y1, y1 = prev, lty = 2, lwd = 2)
abline(lm(y1 ~ x1), lwd = 3, col = cor[2], cex = 2)
```
]


.pull-left[


$$RSS = \sum_{i=1}^{n}(Y_i-\hat{Y}_i)^2 $$

- __Ordinal Least Squares__ minimizes the Residual Sum of Squares to find the best fit for the regression line

 <!-- to obtain the smallest average difference --> 

 
]

---
## The values that minimize RSS


$$\hat{\beta_1}=\frac{\sum_{i=1}^{n}(X_i-\bar{X})(Y_i-\bar{Y})}{\sum_{i=1}^{n}(X_i-\bar{X})^2}$$

$$
\hat{\beta_0} = \bar{Y} - \hat{\beta_1}\bar{X} 
$$


+ These values are __estimates__ ( $\hat{\beta_0}$, $\hat{\beta_1}$, $\hat{\sigma^2}$) of the values of the parameter in the whole population. 

--

+ With different samples, other estimates would be obtained: __the regression line for the sample is not necessarily the same as the regression line for the population__



---
## Variation partition in a linear model

+ The expression ${\sum_{i=1}^{n}(X_i-\bar{X})^2}$ in the denominator of $\hat{\beta_1}$ is the __Sum of Squares__ of variable $X$, a measure of the total variation around its mean

--

+ Likewise, $SS_Y = {\sum_{i=1}^{n}(Y_i-\bar{Y})^2}$ measures the variation of $Y$ around its mean

--

+ $SS_Y$ is in part caused by the regression. The variation that was left unaccounted for by the model is RSS.

$$SS_Y = SS_{Reg} + RSS$$
--

+ __How much of the variation is caused by the regression and how much is due to unaccounted (random) variation?__

---
## Sum of squares (total variation) vs RSS (random variation)

.pull-left[


```{r varpart, echo = F}
plot(y1 ~ x1, data=xy, las=1, bty="l", cex=3, col=cor[3],
     #xaxt="n", yaxt="n",
     xlab="predictor", ylab="response", pch=19, cex.axis = 1.5, cex.lab = 1.5)
abline(h=mean(y1), col=cor[4], lwd=2)
segments(x0=x1, x1=x1, y0=y1, y1=mean(y1), lty=2, lwd = 2)
points(y1 ~ x1, data=xy, las=1, bty="l", cex=.7, pch=19, col=cor[3])
#abline(lm(y1 ~ x1), lwd=2, col='grey30')
```

]

.pull-right[

```{r rss2, echo = FALSE, fig.align='center'}
plot(y1 ~ x1, las = 1, bty = "l", cex = 3, col = cor[3],
     #xaxt="n", yaxt="n",
     xlab = "predictor", ylab = "response",
     pch = 19, cex.axis = 1.5, cex.lab = 1.5)
segments(x0 = x1, x1 = x1, y0 = y1, y1 = prev, lty = 2, lwd = 2)
abline(lm(y1 ~ x1), lwd = 3, col = cor[2], cex = 2)
```
]


---
## Coefficient of determination


$$r^2 = \frac{SS_{reg}}{SS_Y} = \frac{SS_{reg}}{SS_{reg}-RSS}$$

+ The proportion of variation in $Y$ that can be attributed to variation in $X$ (e.g. to the regression). 

--

+ $r^2$ lies between 0 (all variation is random) and 1 (all variation is due to the regression and the points are on the regression line) 

--

+ $r^2$ measures how close to the regression line the points are

--

+ $r^2$ does not confirm causality by itself! __Goodness of fit__

---
class: inverse, center, middle

## (This variation partition is key to understanding the summaries of linear models)

---
## Variation partition summary

<small>

|  Source|  df|  SS sum of squares| MS Mean square | Expected MS | F-ratio | p-value
|:--:|:--:|:--:|:--:|:--:|:--:|:--:|
|  Regression | $1$ | $SS_{reg} = \sum_{i=1}^{n}(\hat{Y}_i-\overline{Y})^2$ | $\frac{SS_{reg}}{1}$ | $\sigma^2+\beta_1^2\sum_{i=1}^{n}X^2$ | $\frac{SS_{reg}/1}{RSS/(n-2)}$ |F-distribution with $n-2$ df| 
|  Residual |  $n-2$ |  $RSS = \sum_{i=1}^{n}(Y_i-\hat{Y}_i)^2$ | $\frac{RSS}{(n-2)}$ | $\sigma^2$| |
|  Total | $n-1$ |  $SS_Y = \sum_{i=1}^{n}(Y_i-\overline{Y})^2$ | $\sigma_Y^2$ |

---
```{r, echo = FALSE, include= F}
mod <- lm(y1 ~ x1)
summary(mod)
```

<!--Report slope, intercept and $r^2$
F-ratio the variance accounted for by the regression un the numerator, the residual variance in the denominator. Larger means more explanation by the regression. P-value low, the chances that this would happen if $H_0$ were true are minimal
-->

<small>

```r
Residuals:
    Min      1Q  Median      3Q     Max 
-5.4855 -2.4053  0.5989  1.7229  3.7023 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)   
(Intercept)   1.4594     2.7122   0.538   0.6072   
x1            3.8456     0.8304   4.631   0.0024 **
---
Signif. codes:  
0 â€˜***â€™ 0.001 â€˜**â€™ 0.01 â€˜*â€™ 0.05 â€˜.â€™ 0.1 â€˜ â€™ 1

Residual standard error: 3.216 on 7 degrees of freedom
Multiple R-squared:  0.7539
Adjusted R-squared:  0.7187 
F-statistic: 21.44 on 1 and 7 DF,  p-value: 0.002396
```

---
## Our example dataset



```{r, echo = F}
kableExtra::kable(data.frame(X = x1, Y = y1))
```

---
## Our example dataset

The example dataset in the current slidedeck was created by simulating data according to the specification of a linear model

$$
Y_i \sim \mathcal{N}(\mu_i, \sigma) \\
$$

$$
\mu_i = \beta_0 + \beta_1 X_i 
$$
With $\beta_0 = 1.2$ and $\beta_1 = 3.5$ and arbitrary x values between 1 and 5:

```{r dataset_creation, collapse = TRUE, eval = F}
set.seed(4)
x1 <- seq(1, 5, by = 0.5)
mu <- 1.2 + 3.5 * x1
y1 <- rnorm(n = 9, mean = mu, sd = 3)
xy <- data.frame(x1, mu, y1, res = y1 - mu)
```



---

## Further reading

+ How Do You Write Your Model Definitions? https://www.sumsar.net/blog/2013/10/how-do-you-write-your-model-definitions/

---

## References

+ Gotelli, N.J., Ellison, A.M., Ellison, S.E. and S.R.F.H.F.A.M., 2004. A Primer of Ecological Statistics. Sinauer Associates Publishers.

+ James, G., Witten, D., Hastie, T., Tibshirani, R., 2013. An Introduction to Statistical Learning: with Applications in R, 1st ed. 2013. Corr. 4th printing 2014 edition. ed. Springer, New York, NY.

+ McElreath, R., 2018. Statistical Rethinking: A Bayesian Course with Examples in R and Stan, 2nd ed. CRC Press.

+ Mortara & SÃ¡nchez-Tapia (2022, July 14). Scientific Programming: Statistical modeling in R (part 1). Retrieved from https://scientific-computing.netlify.app/07_statistical_modeling_1.html




