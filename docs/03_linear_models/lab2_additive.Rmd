---
title: "Multiple linear regression Lab"
author:
 - "Andrea SÃ¡nchez-Tapia"
 - "Paulo InÃ¡cio Prado"
date: "2023-01-13"
output:
    distill::distill_article:
      toc: true
      toc_depth: 3
---

```{r setup, echo=FALSE}
library(rmarkdown)
knitr::opts_chunk$set(eval = FALSE)
# clipboard
htmltools::tagList(
  xaringanExtra::use_clipboard(
    button_text = "Copy code <i class=\"fa fa-clipboard\"></i>",
    success_text = "Copied! <i class=\"fa fa-check\" style=\"color: #90BE6D\"></i>",
    error_text = "Not copied ðŸ˜• <i class=\"fa fa-times-circle\" style=\"color: #F94144\"></i>"
  ),
  rmarkdown::html_dependency_font_awesome()
  )
```

# Introduction 

We can extend the simple linear regression model to accommodate
multiple predictors. These are the **multiple linear regression**
models, of which the simple linear regression is just a particular
case with a single predictor variable.

In the next sections we will show how to fit multiple regression
models with the least square methods in R, and how to interpret the
model coefficients.

# Additive multiple linear model

In a additive multiple linear regression, the expected value of the
response $\mu$ is the sum of multiple predictor variables
$X_i$, each one weighted by its own coefficient
$\beta_i$ :


$$
\begin{align}
Y &\sim \mathcal{N}(\mu, \sigma) \\
\mu & = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \dots + \beta_i X_i \\
\sigma & = C
\end{align}
$$

The expression for the predicted value in this model can be written in the compact form:

$$\mu = \sum \beta_i X_i$$

This is called the **linear predictor**.

## Fitting

We will use a data set of an experiment done with the biennial plant
*Ipomopsis aggregata*^[Crawley 2007. The R Book, Wiley]. Forty plants
were assigned at random to two treatments: unprotected from grazing by
rabbits and protect from grazing by fences. The fruit yield of each
plant (dry mass in mg) was measured at the end of the experiment. To
take into account the effect of the size of the plant on fruit
production, the diameter of the top of rootstock (mm) was also
recorded. Copy the following commands and paste in your R console to
read and prepare this data for the analyses:

```{r , eval=TRUE}
## Data reading and preparation
ipomopsis <- read.csv("https://raw.githubusercontent.com/lmmx/crawley-r-statistics-book-notes/master/code/ipomopsis.csv")
## More proper names
names(ipomopsis) <-  c("diameter","fruit.w","treatment")
## Convert treatment to a factor variable
ipomopsis$treatment <- factor(ipomopsis$treatment)
```

Let's start with a plot of the response variable as function of both
predictors:


```{r }
plot(fruit.w ~ diameter, data = ipomopsis, type ="n",
     ylab = "Fruit dry weight (mg)",
     xlab = "Rootstock diamenter (mm)")
points(fruit.w ~ diameter, data = ipomopsis,
       subset = treatment == "Ungrazed")
points(fruit.w ~ diameter, data = ipomopsis,
       subset = treatment == "Grazed", pch=2, col ="red")
legend("topleft", c("Ungrazed", "Grazed"), pch=c(1,2),
       col =c("black", "red"), bty="n")
```

Larger plants tend to produce a larger dry biomass of fruits. It also
seems that if we control for size, ungrazed plants had larger fruit
yields. We can check this hypothesis by fitting a linear regression
with the additive effect of both plant size and treatment:


```{r , eval=TRUE}
ipo_m1 <- lm(fruit.w ~ diameter + treatment, data = ipomopsis)
```

Use the function `coef` to store the coefficients ($\beta_i$) of this
fitting in a object:


```{r , eval=TRUE}
(ipo_cf1 <- coef(ipo_m1))
```

The predictor variable "treatment" is a factor with two levels
(grazed/ungrazed). The default in R is to create dummy variables for
each level of the factor, except the first one. This means that the
model has a variable *"treatmentUngrazed"*, that has value one for
ungrazed plants, and value zero otherwise. So, fruit yield predicted by
this model for grazed plants is:

$$\mu_{\text{ungrazed}} = \widehat{Y}_{\text{ungrazed}} = \beta_0 + \beta_1 \text{diameter} + \beta_2 \times 0$$

While for grazed plants we have:

$$\mu_{\text{grazed}} = \widehat{Y}_{\text{grazed}} = \beta_0 + \beta_1 \text{diameter} + \beta_2 \times 1$$



To add the line of expected yield for grazed plants we can use the
function `abline`, that draws a straight line with a given intercept and slope:

```{r }
## Make the plot again
plot(fruit.w ~ diameter, data = ipomopsis, type ="n",
     ylab = "Fruit dry weight (mg)",
     xlab = "Rootstock diamenter (mm)")
points(fruit.w ~ diameter, data = ipomopsis,
       subset = treatment == "Ungrazed")
points(fruit.w ~ diameter, data = ipomopsis,
       subset = treatment == "Grazed", pch=2, col ="red")
legend("topleft", c("Ungrazed", "Grazed"), pch=c(1,2),
       col =c("black", "red"), bty="n")
## Adds the predicted line for grazed plants
## check the function help to use it
abline( a = ipo_cf1[1], b = ipo_cf1[2], col = "red")
```

**Challenge:** now add to the plot a black line for the expected yield for
ungrazed plants. Your plot should look like this:


```{r, echo = FALSE, eval =TRUE }
## Make the plot again
plot(fruit.w ~ diameter, data = ipomopsis, type ="n",
     ylab = "Fruit dry weight (mg)",
     xlab = "Rootstock diamenter (mm)")
points(fruit.w ~ diameter, data = ipomopsis,
       subset = treatment == "Ungrazed")
points(fruit.w ~ diameter, data = ipomopsis,
       subset = treatment == "Grazed", pch=2, col ="red")
legend("topleft", c("Ungrazed", "Grazed"), pch=c(1,2),
       col =c("black", "red"), bty="n")
## Adds the predicted line for grazed plants
## check the function help to use it
abline( a = ipo_cf1[1], b = ipo_cf1[2], col = "red")
abline( a = ipo_cf1[1]+ipo_cf1[3], b = ipo_cf1[2])
```

From the plot above, we can see that the difference between the yield
of plants of the same size is a constant, that corresponds to the
value of the coefficient of the variable *"treatmentUngrazed"*.  Thus,
the fitted model estimates that ungrazed plants had on average more `r
round(ipo_cf1[3])` mg of dry mass fruit yield, if we control for plant
size.

**Final Challenge** 

What if we had not controlled for plant size? Fit a
model only with the treatment and interpret the coefficients. 

Hint: it
is good practice to do exploratory data analyses before any model
fitting. In this case a box plot can help, try: 

```{r }
boxplot(fruit.w ~ treatment, data=ipomopsis)
```

## Model diagnostics

Before making inferences with your fitted model, it is also a good
practice to make some model diagnostics. For linear regression, the
most basic diagnostic is to check if the residuals follows a normal
distribution with zero mean and a constant standard deviation. 

Because the normal distribution has a parameter that corresponds to the expected value, and that is independent of the other parameter, the linear regression model :

$$
\begin{align}
Y & \sim \mathcal{N}(\mu, \sigma) \\
\mu & = \sum \beta_i X_i \\
\sigma & = C
\end{align}
$$

Can be also written as:

$$
\begin{align}
Y & = \sum \beta_i X_i + \epsilon \\
\epsilon & \sim \mathcal{N}(\mu, \sigma) \\
\mu & = 0 \\
\sigma & = C
\end{align}
$$

That is, the response $Y$ is a weighted sum of the predictor variables
$X_i$ plus a random Gaussian residual $\epsilon$. To express a random
variation simetrically around the expected value, this residual has mean value of
zero, and a fixed standard deviation.

To check if this assumption is true, we plot the residuals of the
fitted model as a function of the predicted values. this plot should
show a point cloud of constant width around at zero in the Y-axis. You
can check applying the function `plot` to the object that stored the
fitted model:


```{r }
plot(ipo_m1, which =1)
```

You can also check the normality of the residuals with a
qq-plot. Briefly, normal data should lie in a straight line in this
plot:

```{r }
plot(ipo_m1, which =2)
```

You can find an excellent explanation of these and other diagnostic
plots for linear regression
[here](https://data.library.virginia.edu/diagnostic-plots/)
