---
title: "Multiple linear regression and model selection"
subtitle: "Quantitative Ecology Training Program - Serrapilheira"
author: 
  - "PI (Paulo Inácio)"
date: '31 Jan 2024'
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: xaringan-themer.css
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: '16:9'
self-contained: true
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
options(servr.daemon = TRUE)#para que no bloquee la sesión
knitr::opts_chunk$set(eval = TRUE, echo = FALSE, warning = FALSE, message = FALSE)
```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
library(knitr)
style_duo_accent(
  primary_color = "#03045E",
  secondary_color = "#669bbc",
  colors = c(
    red = "#A70000",
    white = "#FFFFFF",
    black = "#181818"
  ),
  text_bold_color = "#03045E",
  header_font_google = google_font("Roboto Condensed"),
  text_font_google = google_font("Roboto Condensed", "300", "300i"),
  code_font_google = google_font("Fira Mono"), text_font_size = "28px"
)
xaringanExtra::use_share_again()
xaringanExtra::use_fit_screen()
xaringanExtra::use_tachyons()
xaringanExtra::use_tile_view()

```


## Key concepts

* Multiple linear regression

* Linear predictor

* Additive predictor

* Predictor with interactions

* Over-fitting and under-fitting

* AIC (for Gaussian models)

* Model selection

---

## Multiple linear regression (Gaussian)

We can extend the simple linear regression model to accommodate
multiple predictors. These are the **multiple linear regression**
models, of which the simple linear regression is just a particular
case with a single predictor variable:

$$\begin{align}
y_i &\sim \text{Normal}(\mu_i, \sigma) \\
\mu_i & = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_j x_{ij} \\
\sigma & = C
\end{align}$$

---

## A quick example to warm up

* **Question**: what's the impact of herbivory on plant fitness?
* Field experiment: 40 plants of *Ipomopsis agreggata* (Pursh)
V.E.Grant assigned at random to two treatments: unprotected from
grazing by rabbits and protect from grazing by fenced cages.
* **Response variable**: fruit yield of each plant (mg dry mass)
* **Predictor variables**: 
  * Treatment:  $0 =$ control (non-fenced), $1 =$ fenced
  * Basal diameter of each plant (mm)
* Data from: Crawley (2007) The R Book.

.center[

```{r, out.width = "50%"}
include_graphics("figs/ipomopsis_and_grazing_cage.png")

```

]


---

## The data

.pull-left[
```{r }
## Data reading and preparation
ipomopsis <- read.csv("https://raw.githubusercontent.com/lmmx/crawley-r-statistics-book-notes/master/code/ipomopsis.csv")
## More proper names
names(ipomopsis) <-  c("diameter","fruit.w","treatment")
## Convert treatment to a factor variable
ipomopsis$treatment <- factor(ipomopsis$treatment,
                              labels = c("Control", "Fenced"))

head(ipomopsis)
```
...

```{r }
tail(ipomopsis)
```
]

.pull-right[
.center[
```{r data, eval=TRUE, echo = FALSE}
par(las=1, mar=c(6,7,4,2)+0.1,
    mgp=c(5,1,0),
    cex.lab=2, cex.axis=1.8, cex.main = 2.25)
plot(fruit.w ~ diameter, data = ipomopsis, type ="n",
     ylab = "Fruit yield dry weight (mg)",
     xlab = "Rootstock diameter (mm)")
points(fruit.w ~ diameter, data = ipomopsis,
       subset = treatment == "Control", pch =19, col="blue")
points(fruit.w ~ diameter, data = ipomopsis,
       subset = treatment == "Fenced", pch = 15, col ="red")
legend("topleft", c("Control", "Fenced"), pch=c(19, 15),
       col =c("blue", "red"), bty="n", cex = 1.5)
```
]]

---

## Fitting multiple regression in R

### The model

$$\begin{align}
\text{Yield}_i &\sim \text{Normal}(\mu_i, \sigma) \\
\mu_i & = \beta_0 + \beta_1 \text{diameter}_{i} + \beta_2 \text{treatment}_{i} \\
\sigma & = C
\end{align}$$



### Call in R

```{r, echo=TRUE}

ipo_m1 <- lm(fruit.w ~ diameter + treatment,
             data = ipomopsis)
```


---

## Coefficients of the fitted model


```{r , echo=TRUE}
summary( ipo_m1 ) 
```




---

## Coefficients of the fitted model

$$\begin{align}
\mu_i & = -128 + 24 \times \text{diameter}_{i} + 36 \times \text{treatment}_{i}
\end{align}$$

### For treatment = 0 (control plants):

$$\begin{align}
\mu_i & = -128 + 24 \times \text{diameter}_{i} + 36 \times 0 \\
      & = -128 + 24 \times \text{diameter}_{i}
\end{align}$$

--

### For treatment = 1 (fenced plants):

$$\begin{align}
	\mu_i & = -128 + 24 \times \text{diameter}_{i} + 36 \times 1 \\
	& = (36 -128) + 24 \times \text{diameter}_{i}
\end{align}$$

---

## Plot with observed and predicted values

.center[

```{r }
ipo_cf1 <- coef(ipo_m1)
par(las=1, mar=c(6,7,4,2)+0.1,
    mgp=c(5,1,0),
    cex.lab=2, cex.axis=1.8, cex.main = 2.25)
plot(fruit.w ~ diameter, data = ipomopsis, type ="n",
     ylab = "Fruit yield dry weight (mg)",
     xlab = "Rootstock diameter (mm)")
points(fruit.w ~ diameter, data = ipomopsis,
       subset = treatment == "Control", pch =19, col="blue")
points(fruit.w ~ diameter, data = ipomopsis,
       subset = treatment == "Fenced", pch = 15, col ="red")
legend("topleft", c("Control", "Fenced"), pch=c(19, 15),
       col =c("blue", "red"), bty="n", cex = 1.5)
abline( a = ipo_cf1[1], b = ipo_cf1[2], col = "blue")
abline( a = ipo_cf1[1] + ipo_cf1[3], b = ipo_cf1[2], col = "red")
```
]

---

## The linear predictor

The expression for the predicted value in multiple linear regression can be written in the compact form:

$$\mu_i = \sum_{j=0}^{J} \beta_{j} x_{ij} \, = \, \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_j x_{ij}$$ 

Where $J$ is the number of predictor variables. This is called the **linear predictor**.

---

## Linearity in the parameters

For statistical models, a predictor is linear if it is a linear
function of parameters, given an observed value of the predictor variables.

--

Is the predictor $\mu_i = \beta_0 + \beta_1 x_{i}^2$ linear in the parameters?

--

**Yes**! For a given observed value of the predictor variable $x_{i}$,
say $x_{i} = 2$, the predictor is 

$$\mu_i = 2 \beta_0 + 4 \beta_1$$

Which is a linear expression.

---

## Some linear predictors

In summary: linear predictors are any weighted sum of
predictor terms. The weights are the linear coefficients. 

--

Each term (or predictor) is any mathematical function of predictor
variables, provided this function does not include the coefficients of
the models. Some examples:

--

$$\begin{align}
\mu_i & = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} +  \beta_3 x_{i3} \\
\mu_i & = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i1}^2 \\
\mu_i & = \beta_0 + \beta_1 \log(x_{i1}) + \beta_2 \sqrt(x_{i2}) \\
\mu_i & = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} +  \beta_3 x_{i1}x_{i2}
\end{align}$$

---

## Non-linear predictors

For statistical models, non-linear predictors result in
non-linear functions of one of more parameters, given the observation of the predictor variables. 

--

For example, the power function $\mu_i = \beta_0 x_{i}^{\beta_1}$ is a non-linear predictor.

--

To see that, take some value for $x_{i1}$, say $x_{i1}=2$. The power function will be 
$\mu_i = \beta_0 \times 2^{\beta_1}$, which is not a linear function of the parameters.

---

## Interactions

We have seen that the linear predictor can include products of predictor variables, like:

$$\begin{align}
y_i &\sim \text{Normal}(\mu_i, \sigma) \\
\mu_i & = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} +  \beta_3 x_{i1} x_{i2}\\
\sigma & = C
\end{align}$$

These products, as the term $x_{i1} x_{i2}$ are called **interactions**.

When two predictor variables interact, their effects are no longer additive.

Rather, the effect of one interacting variable depends on the value of
the other interacting variable (think about the interaction between two drugs).

---

## A simple example of interactions

* **Question**: what are the best soil moisture and light availability to grow tulips?
* Greenhouse experiment: beds of tulips kept in nine combinations of
  soil moisture and shading. Three replicates for each combination
  (total of 27 beds).
* **Response variable**: mean plant height in each bed
* **Predictor variables**: 
  * Watering: $1 =$ low, $2 =$ medium, $3 =$ high
  * Shading: $1 =$ low, $2 =$ medium, $3 =$ high 
* Data from: Grafen & Hails (2002) Modern Statistics for the Life Sciences.

.center[
![](figs\tulips.jpg)
]

---

## The data

.pull-left[

```{r }
tulips <- read.csv("data/tulips.csv")
head(tulips)
```

```{r }
tail(tulips)
```
]

.pull-right[
.center[

```{r }

color <- c("black", "red", "blue")
plot(blooms ~ water, 
     data = tulips, 
     type = "n",
     ylab = "Mean height (mm)",
     xlab = "Water level")
for (s in 1:3) {
points(blooms ~ water, 
       data = tulips,
       subset = shade == s, pch = s, col = color[s])
}
legend("topleft", title = "Shading",
       c("1", "2", "3"),
       pch = c(1, 2, 3), 
       col =c("black", "red", "blue"), bty = "n", cex = 1.2)

```

]]

---

## Fitting a model with interaction in R

### The model

$$\begin{align}
\text{Mean height}_i &\sim \text{Normal}(\mu_i, \sigma) \\
\mu_i & = \beta_0 + \beta_1 \text{watering}_{i} + \beta_2 \text{shading}_{i} + \beta_3 \text{shading}_{i} \text{watering}_{i}\\
\sigma & = C
\end{align}$$


### Fitting in R

```{r , echo = TRUE,}
tulip_int <- lm(blooms ~ water + shade + water:shade,
                data = tulips)
```


---

## Coefficients of the fitted model


```{r, echo=TRUE }
summary(tulip_int) 
```

---

## Coefficients of the fitted model

$$\begin{align}
\mu_i & = -151 + 181 \times \text{water}_{i} + 64 \times \text{shade}_{i} -53 \times \text{shade}_{i} \times  \text{water}_{i}  \\
\end{align}$$

--

###  For shade = 1 :

$$\begin{align}
\mu_i & = -151 + 181 \text{water}_{i} + 64 \times 1 + (-53 \times 1) \text{water}_{i}  \\
      & = (64 -151)  + (181 - 53) \text{water}_{i}
\end{align}$$

--

### For shade = 3:

$$\begin{align}
\mu_i & = -151 + 181 \text{water}_{i} + (64 \times 3) + ( - 53 \times 3) \text{water}_{i}  \\
      & = (192 -151)  + (181 - 159) \text{water}_{i} 
\end{align}$$

---

## Plot with observed and predicted values

.center[

```{r }

tul_cf2 <- coef(tulip_int)
s <- 1:3 #values of shade
intercepts <- tul_cf2[1] + tul_cf2[3] * s
slopes <- tul_cf2[2] + tul_cf2[4] * s


par(las=1, mar=c(6,7,4,2)+0.1,
    mgp=c(5,1,0),
    cex.lab=2, cex.axis=1.8, cex.main = 2.25)
color <- c("black", "red", "blue")
plot(blooms ~ water, 
     data = tulips, 
     type = "n",
     ylab = "Mean height (mm)",
     xlab = "Water level")
for (s in 1:3) {
    points(blooms ~ water, 
           data = tulips,
           subset = shade == s, pch = s, col = color[s])
    abline(a = intercepts[s], slopes[s], col = color[s])
}
legend("topleft",
       c("Shade 1", "Shade 2", "Shade 3"),
       pch = c(1, 2, 3), 
       col = c("black", "red", "blue"), bty = "n", cex = 1.5)
```

]


---

## Inferences about competing models

```{r }
set.seed(4)
df1 <- data.frame(x = runif(21))
df1$y = exp((df1$x-0.3)^2) -1
sim1 <- matrix(rnorm(length(df1$x)*10, mean = df1$y,sd =0.1), ncol = 10)
df1 <- cbind(df1, sim1)
names(df1)[3:12] <- paste("y",1:10, sep="")

f2 <- function(i, data = df1)
    lm(data[,i] ~ x + I(x^2), data = data)

f5 <- function(i, data = df1)
    lm(data[,i] ~ x + I(x^2) + I(x^3) + I(x^4) + I(x^5), data = data)

dg1 <- lapply(3:12, function(i) lm(df1[,i]~x, data = df1))
dg2 <- lapply(3:12, f2)
dg5 <- lapply(3:12, f5)

newdata <- data.frame(x = seq(0,1, length=100))

```

.pull-left[

**The data-generating process (a.k.a. true model):**

$$\begin{align}
y_i &\sim \mathcal{N}(\mu_i, \sigma) \\
\mu_i & = e^{(x_i - 0.3)^2} -1 \\
\sigma & = C
\end{align}$$
]

.pull-right[
.center[

```{r }
par(las=1, mar=c(6,7,4,2)+0.1,
    mgp=c(5,1,0),
    cex.lab=2, cex.axis=1.8, cex.main = 2.25)
plot(y1 ~ x, data=df1, xlab ="X", ylab ="Y", pch=19,
     ylim=c(-.3,0.8))
curve(exp((x-0.3)^2) -1, lwd=1.5, add = TRUE, col="blue")
curve(qnorm(0.025, exp((x-0.3)^2) -1, 0.1), lty=2, add = TRUE, col="blue")
curve(qnorm(0.975, exp((x-0.3)^2) -1, 0.1), lty=2, add = TRUE, col="blue")
```

]]

---

## The models

.pull-left[
$$
\begin{align}
\textbf{M1}: & \mu_i = \beta_0 + \beta_1 x_i \\[1em]
\textbf{M2}: & \mu_i = \beta_0 + \beta_1 x_i + \beta_2 {x_i}^2\\[1em]
\textbf{M3}: & \mu_i = \beta_0 + \beta_1 x_i + \beta_2 {x_i}^2 + \\
&+ \beta_3 {x_i}^3  + \beta_4 {x_i}^4 + \beta_5 {x_i}^5
\end{align}
$$
]

.pull-right[
.center[
```{r }
par(las=1, mar=c(6,7,4,2)+0.1,
    mgp=c(5,1,0),
    cex.lab=2, cex.axis=1.8, cex.main = 2.25)
plot(y1~x, data=df1, xlab ="X", ylab ="Y", pch=19)
lines(predict(dg1[[1]], newdata=newdata)~newdata$x, col ="green")
lines(predict(dg2[[1]], newdata=newdata)~newdata$x, col="blue")
lines(predict(dg5[[1]], newdata=newdata)~newdata$x, col ="red")
legend("topleft", c("Model 1", "Model 2", "Model 3"),
       lty=1, col=c("green", "blue", "red"), bty="n", cex=1.5)
```

]]

---

## Over-fitting

.pull-left[

```{r , echo=TRUE}
M3 <- lm(y1 ~ x + I(x^2) + I(x^3) +
             I(x^4) + I(x^5), data = df1)
summary(M3)$coefficients[,1:2]
```

> To make inferences about unique features of the data at hand, as if they applied to all (or most all) samples (hence the population).
>
.right[Burham & Anderson (2002).]
]

.pull-right[
.center[
```{r }
par(las=1, mar=c(6,7,4,2)+0.1,
    mgp=c(5,1,0),
    cex.lab=2, cex.axis=1.8, cex.main = 2.25)
dg5.p <- lapply(dg5, function(x) predict(x, newdata=newdata)) 
plot(df1[,3]~df1$x, xlab ="X", ylab ="Y", pch=19, type="n",
     ylim = c(min(sapply(dg5.p, min)), max(sapply(dg5.p, max))))
invisible(sapply(dg5.p, function(x) lines(x ~ newdata$x, col ="grey")))
curve(exp((x-0.3)^2) -1, lwd=2, add = TRUE, col="blue")

```
]
]

---

## Under-fitting

.pull-left[

```{r , echo=TRUE}
M1 <- lm(y1 ~ x,
         data = df1)

summary(M1)$coefficients[,1:2]
```


> Failure to identify features in the data-generating process that are
> strongly replicable.
>
.right[Burham & Anderson (2002).]
]

.pull-right[
.center[

```{r }
par(las=1, mar=c(6,7,4,2)+0.1,
    mgp=c(5,1,0),
    cex.lab=2, cex.axis=1.8, cex.main = 2.25)
dg1.p <- lapply(dg1, function(x) predict(x, newdata=newdata)) 
plot(df1[,3]~df1$x, xlab ="X", ylab ="Y", pch=19, type="n",
     ylim = c(min(sapply(dg1.p, min)), max(sapply(dg1.p, max))))
invisible(sapply(dg1.p, function(x) lines(x ~ newdata$x, col ="grey")))
curve(exp((x-0.3)^2) -1, lwd=2, add = TRUE, col="blue")
```
]
]

---

## A good fit


.pull-left[

```{r , echo=TRUE}
M2 <- lm(y1 ~ x + I(x^2), data = df1)

summary(M2)$coefficients[,1:2]
```
]

.pull-right[
.center[
```{r }
par(las=1, mar=c(6,7,4,2)+0.1,
    mgp=c(5,1,0),
    cex.lab=2, cex.axis=1.8, cex.main = 2.25)
dg2.p <- lapply(dg2, function(x) predict(x, newdata=newdata)) 
plot(df1[,3]~df1$x, xlab ="X", ylab ="Y", pch=19, type="n",
     ylim = c(min(sapply(dg2.p, min)), max(sapply(dg2.p, max))))
invisible(sapply(dg2.p, function(x) lines(x ~ newdata$x, col ="grey")))
curve(exp((x-0.3)^2) -1, lwd=2, add = TRUE, col="blue")
```
]
]


---


## Akaike information criterion

.left-column[
.center[
![](../05_maximum_likelihood/figs/Akaike.jpg)
Hirotugu Akaike (1997-2009)
]]

.right-column[


AIC estimates a statistical distance between two probability
distributions: the true one, which is the reference, and a given model
fitted to a sample from the true distribution.

AIC is an estimate of the information of the true data-generating 
distribution that is preserved by a model.

More specifically, AIC is an estimate of the Kullback-Leibler
divergence (or K-L relative entropy) of the model to the reference
true distribution that generated the data.  
]


---

## AIC for Gaussian models


$$\textrm{AIC} \, = \, N \log (\widehat{\sigma^2}) + 2\,K$$

Where $\widehat{\sigma^2}$ is the estimate of the residual variance of
the model, that is, the mean sum of squares of residuals:

$$ \widehat{\sigma^2} \, = \, \frac{1}{N} \, \sum_{i = 1}^N (Y_i - \widehat{Y_i})^2 $$

And $K$ is the number of parameters of the model.





---


## How to use AIC

* AIC expresses distance to the true model, or loss of information by the fitted model;

* Thus, the model with the lowest value of AIC among a set of
  competing models is the most plausible one (or best supported by the
  data);
  
* Canonical rule: models that differ $\leq 2$ in their AIC values are
  equally supported by data;

* To ease model selection, we calculate $\Delta \mathrm{AIC}$: 
$$\Delta_i \, = \, \textrm{AIC}_i - \min (\textrm{AIC})$$

* The best supported , or more plausible, model will have $\Delta_i = 0$

---

## AICc: correction for small samples

For  $n/K \, < \,40$, multiply $K$ by the correction term


$$ \left(\frac{n}{n-K-1}\right) $$

### AICc for Gaussian models:

$$ \textrm{AICc} \, = \, -2\,\textbf{L}_y(\widehat{\theta}) + 2\,K \left(\frac{n}{n-K-1}\right) $$


Where $n$ is the sample size.


---

## Evidence weights 

$$w_i\,=\, \frac{e^{-1/2 \Delta_i}}{\sum e^{-1/2 \Delta_i}}$$

* Evidence or Akaike weights sum up one;

* Thus, $w_i$ express the relative support of each model in the set of
  competing models, in a standardized scale;

* In a frequentist approach, $w_i$ estimates the probability that each
  model will be the best supported one, if you repeat the sample and then the selection many
  times.


---

## Model selection with AIC: example

.pull-left[


```{r , echo=TRUE}
library(bbmle)
M1 <- lm(y1 ~ x, data = df1)
M2 <- lm(y1 ~ x + I(x^2), data = df1)
M3 <- lm(y1 ~ x + I(x^2) + I(x^3) +
             I(x^4) + I(x^5), data = df1)

AICctab(M1, M2, M3, logLik=TRUE, base = TRUE,
        weights = TRUE, nobs = nrow(df1))
```

]


.pull-right[
.center[

```{r }
par(las=1, mar=c(6,7,4,2)+0.1,
    mgp=c(5,1,0),
    cex.lab=2, cex.axis=1.8, cex.main = 2.25)
plot(y1~x, data=df1, xlab ="X", ylab ="Y", pch=19)
lines(predict(dg1[[1]], newdata=newdata)~newdata$x, col ="green")
lines(predict(dg2[[1]], newdata=newdata)~newdata$x, col="blue")
lines(predict(dg5[[1]], newdata=newdata)~newdata$x, col ="red")
legend("topleft", c("Model 0", "Model 1", "Model 2"),
       lty=1, col=c("green", "blue", "red"), bty="n", cex=1.5)
```
]]


---

## Final remarks on AIC

   * Ties can and do happen: more than one model with $\Delta_i < 2$
   tell us that the data does not contain enough evidence to spot
   the best model;
   
   * Model selection with AIC is not a statistical test; 
   
   * Model selection is restricted to competing models: if all
     competing models are bad, the selected model will just be the
     least bad.
   
   * AIC does not express goodness of fit. The selected model can still have a poor fit; 
   
   * AIC cannot be used to compare models fitted to different datasets; 
   
   * For the same reason, the AIC cannot be used to compare models
     fitted to transformed and untransformed data.
   
---



## Further reading

+ Burnham, K. P., & Anderson, D. R. (2002). Model Selection and
  Multimodel Inference: A Practical-Theoretic Approach, 2nd ed. New
  York, Springer-Verlag.

+ Bolker, B.M. 2008 Ecological
  Models and Data in R. Princeton: Princeton University Press.

+ McElreath, R., 2018. Statistical Rethinking: A Bayesian Course with
  Examples in R and Stan, 2nd ed. CRC Press.
  
  
