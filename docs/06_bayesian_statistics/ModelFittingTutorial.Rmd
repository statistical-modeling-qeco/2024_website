---
title: "Bayesian Model Fitting Tutorial"
description: |
author: "Diogo Melo"
date: '2023-01-20'
output: 
    distill::distill_article
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Installing the rethinking package

We are going to use rethinking, a simpler model fitting package designed for a teaching environment. If you have rethinking installed, jump to the next section.

First we need to install rstan:

```{r, eval = FALSE}
install.packages("rstan", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))

```

Next, the rethinking package:

```{r, eval = FALSE}
install.packages(c("coda","mvtnorm","remotes", "loo", "bayesplot"))
library(remotes)
remotes::install_github("rmcelreath/rethinking")
```

cmdstanR is also a great way to have a more stable R environment:

```{r, eval = FALSE}
# we recommend running this is a fresh R session or restarting your current session
install.packages("cmdstanr", repos = c("https://mc-stan.org/r-packages/", getOption("repos")))
```

Next, install cmdstan:

```{r, eval = FALSE}
install_cmdstan()
```

Now we should be good to go!

# Defining a very simple posterior

Consider the following model:

$$
\begin{aligned}
y_i &\sim Normal(\mu, \sigma) \\
\mu &\sim Normal(0, 1) \\
\sigma &\sim Exponential(1)
\end{aligned}
$$

Can we write this posterior in R? Sure!

```{r}
set.seed(2)
# Data:
y = rnorm(100, mean = 2, sd = 1)

# Posterior, in log scale for numerical stability
logP_theta_y = function(mu, sigma = 1, data = y){
  ll = sum(dnorm(data, mu, sigma, log = TRUE))
  logprior = dnorm(mu, 0, 1, log = TRUE) + dexp(sigma, 1, log = TRUE)
  logprior + ll # P(theta)P(y|theta)
}
x = seq(0, 4, 0.01)
px = Vectorize(logP_theta_y)(x)
plot(px ~ x, pch = 19, col = 2, xlab= "mean parameter (mu)", ylab = "proportional to P(mu|y)")
abline(v = 2)
```

In this univariate case it's easy to check many possible values of $\mu$ to find the high probability values, but in high dimensions this is not feasible.

# Defining a model in the rethinking package

The rethinking syntax closely mirrors the mathematical notation we have been using in class.
For example, a simple linear regression of $y_i$ on $x_i$ can be defined as, using some standard-ish priors:

$$
\begin{aligned} 
  y_i &\sim Normal(\mu_i, \sigma) \\
  \mu_i &= \alpha + \beta x_i  \\
  \alpha &\sim Normal(0, 1) \\
  \beta &\sim Normal(0, 1) \\
  \sigma &\sim Exponential(1) \\
\end{aligned} 
$$
This translates to the following code, including some simulated data:

```{r, eval = TRUE, message=FALSE, warning=FALSE}
library(rethinking)
N = 100 # sample size

# Linear regression parameters
alpha = 1 # Intercept
beta = 1  # slope on x
 
x = rnorm(N)           # Simulated independent variable. Mean = 0 and sd = 1
mu = alpha + beta * x  # linear model for the expected value
y = rnorm(N, mu)       # Response variable, with residual sd = 1

# Rethinking code
m1 = ulam(alist(
  y ~ normal(mu, sigma),     # Likelihood
  mu <- a + b * x,           # Linear model
  a ~ normal(0, 1),          # Prior on intercept
  b ~ normal(0, 1),          # Prior on slope
  sigma ~ exponential(1)),   # Prior on residual variance
  data = list(y = y, x = x), # Data list
  iter = 2000, chains = 4, cores = 4, refresh = 0) # Sampler control parameters
precis(m1, digits = 2) # Summary of model fit
```

The precis() function generates a simple summary table of fitted coefficients. 

__Exercise:__ Fit the same model using the lm() function in R, and compare the summary of the two fitted models. 

If the sample size is large, the likelihood overwhelms the prior and even strong priors can lead to similar inferences.

__Exercise:__ Change the priors. Can you find a prior that substantially changes the estimates? Next, reduce the sample size (change the N variable). Do the priors make a difference now?

## Getting the generated Stan code

Some models are too complicated to fit in the simple rethinking syntax. For these models we have to write Stan code directly. We can start to see how these models are structured by looking at the Stan code rethinking generates.

```{r, fold = TRUE}
writeLines(m1@model)
```

Note the different code blocks for declaring the different parts of the model (data, parameters, and model). Also note the need to declare all variables and their domain constraints. We will see more complicated Stan models later in the course.

## Checking convergence

Running multiple independent MCMC chains allows us to check if the chains converged
to the same parameter values. Ideally, all chains should jump around the same values, without
getting stuck on some value and without much similarity between subsequent values. Let's plot the 4 chains:

```{r}
traceplot_ulam(m1)
```

These look fine. Notice the initial jumps far away from the typical set. This is the sampler's initial warm-up phase, before the chains converge.

There are also diagnostics calculated on multiple chains to check for convergence.
The most common one is the R-hat diagnostic, which measures the ratio of the within and between chain variance. If the chains converged to the same value, R-hat should be very close to 1. The precis() summary outputs the R-hat for all parameters by default.

There are other more advanced diagnostics in Stan models, which can be very powerful and allow us to identify ill behaved models. See this [post](https://mc-stan.org/misc/warnings.html) for more.

## Ploting the posterior

When we are using simulated data, we can compare the posterior estimates to the known parameters. The [bayesplot package](https://mc-stan.org/bayesplot/) provides several functions for visualizing the posterior:

```{r}
library(bayesplot)
samples = as.data.frame(extract.samples(m1))
mcmc_recover_hist(samples[,c("a", "b")] , true = c(alpha, beta))
```

Sometimes the mean of the posterior does not match the simulated value exactly. This is normal for any single run so long as the simulated value is in a reasonably high probability region of the posterior. A more thorough check would be to run the simulation several times and to check model is properly calibrated. For example, we might check if the true value falls into the 50% confidence region 50% of the runs.

__Challenge exercise__: Make a for loop to check model calibration.

## Ploting the data and the regression

Now we can evaluate the model fit to the simulated data. First, let's draw a few regression lines from the posterior. This will give us some idea about the uncertainty in the regression line.

```{r}
plot(y~x, pch = 19)
for(i in 1:30)
  abline(a = samples[i,"a"], b = samples[i,"b"], col = adjustcolor(2, alpha = 0.3))
```

We can also include the mean posterior estimate of the regression line:

```{r}
# Posterior mean estimates of the parameters
coef = colMeans(samples)

plot(y~x, pch = 19)
abline(a = coef["a"], b = coef["b"], col= 2, lwd = 2)
```

While looking at the fitted model against the data is usually more informative, we can also plot the coefficient estimates directly. It is sometimes difficult to interpret coefficients directly, but with some practice numerical estimates can be useful in inference.

A basic confidence interval plot can be generated by calling the plot function on the precis() output table:

```{r}
plot(precis(m1), xlim =c(-0.1, 1.5))
```


## Posterior predictive checks

One powerful concept in Bayesian models is of simulated data from the posterior. Ideally, we would like the posterior distribution to generate simulated data that is compatible with the data used to fit the model. If we can not reproduce some relevant aspect of the data given the fitted model, it is usually a sign that the model needs to be modified.

Here, we generate 30 simulated data sets from the posterior:

```{r}
n_samples = 30
simulated_data = array(NA, dim = c(N, n_samples))
for(k in 1:n_samples)
  simulated_data[,k] = rnorm(N, 
                             mean = samples[k, "a"] + samples[k, "b"] * x, 
                             sd = samples[k, "sigma"])

plot(y~x, pch = 19)
for(k in 1:n_samples)
  points(simulated_data[,k]~x, col = adjustcolor(2, alpha = 0.2))

```

One useful technique is to compare some tail percentile from the posterior predictive distribution to the observed percentile. This can identify problems at the extreme tails of the data distribution, where it's harder to get a good fit.

```{r}
# Comparing the predicted 90% percentile to the observed
simualated_quantile = apply(simulated_data, 2, quantile, 0.9)
dens(simualated_quantile, col = 2, lwd = 4) # density of simulated 90% percentiles
abline(v = quantile(y, 0.9), lwd = 3)       # observed 90% percentile
```

