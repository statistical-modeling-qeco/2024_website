<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Maximum likelihood fitting</title>
    <meta charset="utf-8" />
    <meta name="author" content="PI (Paulo InÃ¡cio Prado)" />
    <meta name="date" content="2024-02-02" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <script src="libs/clipboard/clipboard.min.js"></script>
    <link href="libs/shareon/shareon.min.css" rel="stylesheet" />
    <script src="libs/shareon/shareon.min.js"></script>
    <link href="libs/xaringanExtra-shareagain/shareagain.css" rel="stylesheet" />
    <script src="libs/xaringanExtra-shareagain/shareagain.js"></script>
    <script src="libs/xaringanExtra_fit-screen/fit-screen.js"></script>
    <link href="libs/tachyons/tachyons.min.css" rel="stylesheet" />
    <link href="libs/tile-view/tile-view.css" rel="stylesheet" />
    <script src="libs/tile-view/tile-view.js"></script>
    <link href="libs/xaringanExtra-clipboard/xaringanExtra-clipboard.css" rel="stylesheet" />
    <script src="libs/xaringanExtra-clipboard/xaringanExtra-clipboard.js"></script>
    <script>window.xaringanExtraClipboard(null, {"button":"Copy code <i class=\"fa fa-clipboard\"><\/i>","success":"Copied! <i class=\"fa fa-check\" style=\"color: #90BE6D\"><\/i>","error":"Not copied ðŸ˜• <i class=\"fa fa-times-circle\" style=\"color: #F94144\"><\/i>"})</script>
    <link href="libs/font-awesome/css/all.min.css" rel="stylesheet" />
    <link href="libs/font-awesome/css/v4-shims.min.css" rel="stylesheet" />
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Maximum likelihood fitting
]
.subtitle[
## Quantitative Ecology Training Program - Serrapilheira
]
.author[
### PI (Paulo InÃ¡cio Prado)
]
.date[
### 02 Feb 2024
]

---






##Recap

* Statistical models describe the distribution of probabilities of
  random variables.

--

* This is done by turning parameters of probability distributions into
  functions of predictor variables.

--

* In the simple linear regression model, a response `\(y_i\)` is described
  as a Gaussian random variable, with a location parameter `\(\mu\)` as a
  linear function of a variables `\(x_i\)`:
  
$$
`\begin{align}
y_i &amp;\sim \mathcal{N}(\mu_i, \sigma) \\
\mu_i &amp; = \beta_{0} + \beta_1 x_i
\end{align}`
$$

---

##Recap

* The least squares method (LS) is a way to fit the linear
  regression model, by minimizing the sum of the squared residuals.

--

* LS estimators of the parameters of the linear predictor for a simple
  linear regression are:

$$
`\begin{align}
\\
\hat{\beta_1}&amp;=\frac{\sum(x_i-\bar{x})(y_i-\bar{y})}{\sum (x_i - \bar{x})^2 }\\[2em]
\hat{\beta_0}&amp;=\bar{y}-\hat{\beta_1}\bar{x}
\end{align}`
$$

---

##Today we'll see ...

* that LS is a rather good idea to fit Gaussian models, as it is a
particular case of the **Maximum Likelihood Method** (ML)

--

* that ML provides a general criterium to find the parameter values that
  are best supported by data, for any statistical model.

--

* how ML estimates (MLEs) can be calculated (or how we can fit models
  to data with the ML method, which is the same).

---

##Our Good Ol'friend, the linear regression

.pull-left[

$$
`\begin{align}
y_i &amp;\sim \mathcal{N}(\mu_i, \sigma) \\
\mu_i &amp; = \beta_0 + \beta_i x_i \\
\sigma &amp; = C \\
\end{align}`
$$

]


.pull-right[
.center[
![](slides_files/figure-html/unnamed-chunk-1-1.png)&lt;!-- --&gt;
]
]

---

##Models assign probabilities to observations

.pull-left[

$$
`\begin{align}
y_i &amp;\sim \mathcal{N}(\mu_i, \sigma) \\
\mu_i &amp; = \beta_0 + \beta_i x_i \\
\sigma &amp; = C \\
\end{align}`
$$

### Example: 

What's the Normal density for `\(\{x=0.5, y=32\}\)`?

]

.pull-right[
.center[
![](slides_files/figure-html/unnamed-chunk-2-1.png)&lt;!-- --&gt;
]
]

---

##Models assign probabilities to observations

.pull-left[

$$
`\begin{align}
y_i &amp;\sim \mathcal{N}(\mu_i, \sigma) \\
\mu_i &amp; = \beta_0 + \beta_i x_i \\
\end{align}`
$$

### Example: 
Normal density for `\(\{x=0.5, y=32\}\)`:


```r
(mu &lt;- 10 + 30*0.5)
```

```
## [1] 25
```

```r
dnorm(32, mean = mu, sd = 5)
```

```
## [1] 0.02994549
```

]

.pull-right[
.center[

![](slides_files/figure-html/unnamed-chunk-4-1.png)&lt;!-- --&gt;
]
]

---


## Statistical models are hypotheses ... 

.center[

.pull-left[
![](slides_files/figure-html/unnamed-chunk-5-1.png)&lt;!-- --&gt;
]

.pull-right[
![](slides_files/figure-html/unnamed-chunk-6-1.png)&lt;!-- --&gt;
]
]

---

## ... about the process that generated the data

.center[

.pull-left[
![](slides_files/figure-html/unnamed-chunk-7-1.png)&lt;!-- --&gt;
]]

.center[
.pull-right[
![](slides_files/figure-html/unnamed-chunk-8-1.png)&lt;!-- --&gt;
]]

---

##Likelihood: a measure of support

### The likelihood law (informal statement)

.bg-washed-blue.b--navy.ba.bw2.br3.shadow-5.ph4.mt1[
.center[**IF**]

* There are many hypotheses or models to describe a given data set, 
* and each model assigns different probabilities to data
]

--

.center[
.bg-washed-blue.b--navy.ba.bw2.br3.shadow-5.ph4.mt1[
**THEN**

The model that assigns the larger probability to data is the best
supported by the data, or the more plausible model.  ]
]

---

##Likelihood law: a simple example

.pull-left[
**Two hypotheses**

* `\(H_A\)`: the balls in the urn are all red
* `\(H_B\)`: half of the balls are red and half are white

**Data collecting**

Draw balls at random from the urn, with replacement
]

.center[
.pull-right[
&lt;img src="figs/urn2.png" width="324" /&gt;
]]

---

##Likelihood law: a simple example

### A single drawn, and the ball is red

$$
`\begin{align}
\\
P(\text{red} \mid H_A) &amp; = 1\\
P(\text{red} \mid H_B) &amp; = 0.5
\\
\end{align}`
$$

`\(H_A\)` is `\(\frac{1}{0.5}\; = \; 2\)` times more plausible (or better
supported by the observation) than `\(H_B\)`.

---


##Likelihood law: a simple example

### Two draws, both of red balls

$$
`\begin{align}
\\
P(\{\text{red, red}\} \mid H_A) &amp; = 1 \times 1 = 1\\
P(\{\text{red, red}\} \mid H_B) &amp; = 0.5 \times 0.5 = 0.25
\\
\end{align}`
$$

`\(H_A\)` is `\(\frac{1}{0.25}\; = \; 4\)` times more plausible (or better
supported by the observations) than `\(H_B\)`. &lt;sup&gt;1&lt;/sup&gt; 

.footnote[[1] under the assumption that the data are independent observations]

---

##The likelihood function

Any function proportional to the product of probabilities assigned by
a statistical model to each observation in data:


.bg-washed-blue.b--navy.ba.bw2.br3.shadow-5.ph4.mt1[
$$ \mathcal{L} \propto P(y_1 \mid H) \times P(y_2 \mid H) \times \ldots P(y_i \mid H)$$
]


---

##The likelihood function

Any function proportional to the product of probabilities assigned by
a statistical model to each observation in data:


.bg-washed-blue.b--navy.ba.bw2.br3.shadow-5.ph4.mt1[
$$ \mathcal{L} \propto \prod^N P(y_i \mid H)$$
]

---

##The log-likelihood function

The logarithm of the likelihood function, that is, any function
proportional to the **sum of the probabilities** assigned by a model
to each observation in data:

.bg-washed-blue.b--navy.ba.bw2.br3.shadow-5.ph4.mt1[
$$ \textbf{L} \propto \sum^N \ln P(y_i \mid H)$$
]


---

##Important remarks

**The likelihood is a function of the model parameters:** likelihoods
are conditional to a given data set. That is, data is taken as an
invariant, while the model parameters can change.

--

**Likelihood functions are not probability distribution functions**,
though you need probabilities distributions to calculate likelihoods.

--

**Notation** 

We will use to notation `\(L_y(\theta)\)` to express:

* a likelihood function conditional to a given data set `\(y = {y_1, \ldots, y_n}\)` ,

* for a model that has a set of parameters `\(\theta = {\theta_1, \ldots, \theta_i}\)`.

---

##Likelihood function of simple linear regression

The probability density function of the Gaussian for a given observation `\(y_i\)` is

`$$\mathcal{N}(y_i) \, = \, (2 \pi \sigma^2)^{-\frac{1}{2}} \, e^{-\frac{1}{2\sigma^2}  (y_i - \mu_i)^2}$$`

--

In the linear regression, `\(\mu_i = \beta_0 + \beta_1 x_i\)`. Substituting we have:

`$$\mathcal{N}(y_i) \, = \, (2 \pi \sigma^2)^{-\frac{1}{2}} \, e^{-\frac{1}{2\sigma^2}  (y_i - (\beta_0 + \beta_1 x_i))^2}$$`

--

Thus, the parameter set of the simple linear regression is 
`\(\theta =\{ \beta_0, \beta_1, \sigma \}\)` 
and the likelihood function is the
product of the density function defined above over all values of
`\(y_i\)`:

`$$\mathcal{L}_y (\theta) \, = \, \prod_{i=1}^n \, (2 \pi \sigma^2)^{-\frac{1}{2}} \, e^{-\frac{1}{2\sigma^2}  (y_i - (\beta_0 + \beta_1 x_i))^2}$$`


---

##Example of likelihood for one parameter

.pull-left[
.center[
![](slides_files/figure-html/unnamed-chunk-10-1.png)&lt;!-- --&gt;
]]

.pull-right[
.center[

![](slides_files/figure-html/unnamed-chunk-11-1.png)&lt;!-- --&gt;
]
]

---

##Example of likelihood for one parameter

.pull-left[
.center[
![](slides_files/figure-html/unnamed-chunk-12-1.png)&lt;!-- --&gt;
]]

.pull-right[
.center[

![](slides_files/figure-html/unnamed-chunk-13-1.png)&lt;!-- --&gt;
]
]

---

##The method of maximum likelihood

  When you choose one or more statistical models to describe data,
  you do not know the values of the model parameters.

.bg-washed-blue.b--navy.ba.bw2.br3.shadow-5.ph4.mt1[
  **Fitting by the ML method**: 
  
  To find the values of the parameters that are best supported by
  data, that is, the values of the parameters that maximize the
  likelihood function:
  
  `$$\widehat{\theta} =  \underset{\theta}{\mathrm{argmax}}[L_y(\theta)]$$`
]


---

##Simple linear regression: MLEs deduction 

### Log-likelihood function of the model:

$$
`\begin{align}
&amp; \textbf{L}_y(\mu,\beta_0,\beta_1) = \\[1em] 
&amp;\ln [ \mathcal{L}_y (\mu,\beta_0,\beta_1) ] = \\[1em]
&amp; \ln \left[ \prod_{i=1}^n \, (2 \pi \sigma^2)^{-\frac{1}{2}} \, e^{-\frac{1}{2\sigma^2}  (y_i - (\beta_0 + \beta_1 x_i))^2} \right] = \\[1em]
&amp; \frac{n}{2} \ln(2 \pi) \, + \, \frac{n}{2} \ln(2 \sigma^2) \, + \, \frac{1}{2 \sigma^2}\, \sum_{i=1}^n (y_i - \beta_0 -\beta_i x_i)^2
\end{align}`
$$


---
##Simple linear regression: MLEs deduction 

### Take derivatives and set equal to zero

$$
`\begin{align}
\frac{\partial \textbf{L}_y(\beta_0,\beta_1,\sigma^2)}{\partial \beta_0} =  0 &amp; \implies \sum y_i \, = \, n \beta_0 + \beta_1 \sum x_i \\[1em]
\frac{\partial \textbf{L}_y(\beta_0,\beta_1,\sigma^2)}{\partial \beta_1} = 0 &amp; \implies \sum y_i x_i \, = \, \beta_0 \sum x_i + \beta_1 \sum x_i^2 \\[1em]
\frac{\partial \textbf{L}_y(\beta_0,\beta_1,\sigma^2)}{\partial \sigma^2} =  0 &amp; \implies n - \frac{1}{\sigma^2} \sum (y_i - \beta_0 - \beta_1)^2 \, = \, 0
\end{align}`
$$

---

##LS method is the maximum likelihood solution for Gaussian models!

### Solving the system with the first two equations:

$$
`\begin{cases}
\sum y_i   =  n \beta_0 + \beta_1 \sum x_i \\[1em] 
\sum y_i x_i  =  \beta_0 \sum x_i + \beta_1 \sum x_i^2
\end{cases}`
$$

### We get:

$$
\hat{\beta_1}\, =\, \frac{\sum(x_i-\bar{x})(y_i-\bar{y})}{\sum (x_i - \bar{x})^2 } \; \;  , \; \;  
\hat{\beta_0}\, = \, \bar{y}-\hat{\beta_1}\bar{x} 
$$


---

##Some other analytical MLEs

To fit distributions to data (very simple models, no predictors):

* Poisson: `\(\; \hat{\lambda} \, = \, \frac{\sum{y_i}}{n}\)` 

* Normal: `\(\; \hat{\mu} \, = \, \frac{\sum{y_i}}{n}\)` , `\(\; \hat{\sigma} \, = \, \frac{\sqrt{(y_i - \bar{x})^2}}{n}\)`

* Binomial: `\(\; \hat{p} \, = \, \frac{y}{N}\)`

* Exponential: `\(\; \hat{\lambda} \, = \, \frac{n}{\sum{y_i}}\)`

* Geometric: `\(\; \hat{p} \, = \, \frac{n}{n + \sum y_i}\)`

---

##Example: Poisson ML fitting


.pull-left[
.center[
![](./figs/euterpe_edulis.jpeg)
]

* 568 plots in the Atlantic Forest
* A total of 4242 palm trees recorded.
* To fit a Poisson, we need just the sample mean:

$$ \hat{\lambda}  =  \frac{4242~\text{trees}}{568~\text{plots}}   =  7.468/ \text{plot} $$

]

.center[
.pull-right[
![](slides_files/figure-html/euterpe-data-1.png)&lt;!-- --&gt;
]]


---

##Example: Poisson ML fitting

.center[
![](slides_files/figure-html/unnamed-chunk-14-1.png)&lt;!-- --&gt;
]

---

##Poisson x Negative Binomial

.pull-left[

**Which model is best supported by this data?**

* Log-Likelihoods differences: 

`$$L_{NB} - L_{Poisson} =  1924$$`

* That is, a likelihood ratio of `\(e^{1924}\)`

* The negative binomial fit is overly best supported by the data, **compared** to the Poisson fit.
]

.pull-right[
.center[
![](slides_files/figure-html/unnamed-chunk-15-1.png)&lt;!-- --&gt;
]]


---

##Finding MLEs numerically

.pull-left[

* In most cases analytical solutions for MLEs are not available. 

* In part because even relatively simple models have more than 3
parameters and thus, **complex, multidimensional likelihood
surfaces**.

* We have to rely on numerical methods, usually
using computers (optimization routines).
] 

.pul-right[
.center[
[![](./figs/optimization.gif)](https://towardsdatascience.com/a-visual-explanation-of-gradient-descent-methods-momentum-adagrad-rmsprop-adam-f898b102325c)
]]

???

In most cases a analytical solution is not available. In part because even simple models have more than 3 parameters, linked by complicated functions. We have to rely on numerical methods, usually using computers

Figure of LL surface of a Gaussian and of the contour map with optimization route
https://towardsdatascience.com/a-visual-explanation-of-gradient-descent-methods-momentum-adagrad-rmsprop-adam-f898b102325c

---

##Optimization is tricky

.pull-left[
.center[
[![](./figs/one_thing.png)](https://www.youtube.com/watch?v=_U-7L1tmBAo)
]
]

.pull-right[
.center[
[![](./figs/two_optima.gif)](https://towardsdatascience.com/a-visual-explanation-of-gradient-descent-methods-momentum-adagrad-rmsp)
]]


---


##Look for ML fitting in good libraries

* There are libraries for ML fitting of a huge variety of models.

* To start using a class of statistical models, look carefully for
  stable and affordable software. Advice of more experienced users is the 
  safest path.

* Good libraries embody the expertise of collaborating statisticians,
  computer scientists and users, *e.g.*:

  * Linear regression LS: `stats::lm()`
  * Non-linear LS: `stats::nls`
  * Generalized linear models: `stats::glm()`
  * Mixed effect Gaussian models : `lme4::lmer`
  * Mixed effects glm's: `lme4::glmer`
  * ...


---

##Properties of MLEs

### as `\(n \rightarrow \infty\)` :

* **Consistency**: converges to the value of the parameter to be estimated, in probability.

* **Efficiency**: has the lowest possible mean-square error, for a consistent estimator. 

* **Normality**: the sample distribution of MLE's asymptotically tends to normality.&lt;sup&gt;2&lt;/sup&gt; 

### In general:

* **Functional invariance**: a monotonic function of a MLE is also a MLE.


.footnote[[2] A generalization of Central Limit Theorem, follows from the efficiency property]

---

##Inference about parameters 



.pull-left[

### Log-likelihood profiles

* A plot of the minimum of the **negative log-likelihood** value as a
function of a single parameter;

* Y-axis standardized such as its minimum is zero.

* **Canonical rule**: all parameter values within 2 units of the relative
  negative log-likelihood are equally plausible.

]

.pull-right[
.center[
![](slides_files/figure-html/unnamed-chunk-17-1.png)&lt;!-- --&gt;
]]

---

##Danger Will Robinson!

.pull-left[
.center[
[![](./figs/one_thing.png)](https://www.youtube.com/watch?v=_U-7L1tmBAo)
]
]

.pull-right[
.center[
![](figs/amostra100.png)
]]

---

##Example: scaling of parasite loads

.pull-left[
* The biomass of bird ectoparasites is proportional to the `\(2/3\)`
power of host body mass.
* Data: Average body biomass and average ectoparasites mass per bird for 42
   bird species.
* Simple linear regression of log of average parasite biomass as the response, and log of average body mass as the predictor variable.
 
.footnote[Hechinger et al. Proc. R. Soc, 2019]
 ]


.pull-right[
.center[
![](slides_files/figure-html/unnamed-chunk-18-1.png)&lt;!-- --&gt;
]]

---

##Plausibility intervals

.pull-left[
.center[

![](slides_files/figure-html/unnamed-chunk-19-1.png)&lt;!-- --&gt;
]
]

.pull-right[
.center[
![](slides_files/figure-html/unnamed-chunk-20-1.png)&lt;!-- --&gt;
]]

---

## Take-home messages

+ Any statistical model hypothesis on how data has been generated. As
  such, these hypotheses assigns probabilities to data.

--

+ Likelihood law: stick with the model that assigns the larger probability to your data

--

+ The (log) likelihood expresses the support of the data for a given
  model, **compared** to other models fitted to the same data set.

--

+ We can fit statistical models by finding the values of the
  parameters that maximize the model likelihood.

--

+ Maximum likelihood estimates have many nice statistical properties.

---

## Further reading

+   Likelihood and all that. chapter 6 *in* Bolker, B.M. 2008
    Ecological Models and Data in R. Princeton: Princeton University
    Press.

+   Hobbs, N.T. &amp; Hilborn, R. 2006. Alternatives to statistical
    hypothesis testing in ecology: A guide to
    self-teaching. Ecological Applications: 16(1): 5-19. 

+   The  Concept of Likelihood , chapter 2 *in*: Edwards, A.W.F., 1992
    Likelihood. Baltimore: John Hopkins University Press.  

    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
