---
title: "Maximum likelihood fitting"
subtitle: "Quantitative Ecology Training Program - Serrapilheira"
author: 
  - "PI (Paulo InÃ¡cio Prado)"
date: '02 Feb 2024'
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: xaringan-themer.css
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      ratio: '16:9'
self-contained: true
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
options(servr.daemon = TRUE)#para que no bloquee la sesiÃ³n
knitr::opts_chunk$set(eval = TRUE, echo = FALSE, warning = FALSE, message = FALSE)
```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
library(ggplot2)
library(ggthemes)
library(dplyr)
library(MASS)
library(sads)
style_duo_accent(
  primary_color = "#03045E",
  secondary_color = "#669bbc",
  colors = c(
    red = "#A70000",
    white = "#FFFFFF",
    black = "#181818"
  ),
  text_bold_color = "#03045E",
  header_font_google = google_font("Roboto Condensed"),
  text_font_google = google_font("Roboto Condensed", "300", "300i"),
  code_font_google = google_font("Fira Mono"), text_font_size = "28px"
)
xaringanExtra::use_share_again()
xaringanExtra::use_fit_screen()
xaringanExtra::use_tachyons()
xaringanExtra::use_tile_view()
# clipboard
htmltools::tagList(
  xaringanExtra::use_clipboard(
    button_text = "Copy code <i class=\"fa fa-clipboard\"></i>",
    success_text = "Copied! <i class=\"fa fa-check\" style=\"color: #90BE6D\"></i>",
    error_text = "Not copied ðŸ˜• <i class=\"fa fa-times-circle\" style=\"color: #F94144\"></i>"
  ),
  rmarkdown::html_dependency_font_awesome()
  )
## ggplot theme
theme_Publication <- function(base_size=14, base_family="helvetica") {
    (theme_foundation(base_size=base_size, base_family=base_family)
        + theme(plot.title = element_text(face = "bold",
                                          size = rel(1.2), hjust = 0.5),
                text = element_text(),
                panel.background = element_rect(colour = NA),
                plot.background = element_rect(colour = NA),
                panel.border = element_rect(colour = NA),
                axis.title = element_text(face = "bold",size = rel(1)),
                axis.title.y = element_text(angle=90,vjust =2),
                axis.title.x = element_text(vjust = -0.2),
                axis.text = element_text(), 
                axis.line = element_line(colour="black"),
                axis.ticks = element_line(),
                panel.grid.major = element_line(colour="#f0f0f0"),
                panel.grid.minor = element_blank(),
                legend.key = element_rect(colour = NA),
                legend.position = "bottom",
                legend.direction = "horizontal",
                legend.key.size= unit(0.2, "cm"),
                ##legend.margin = unit(0, "cm"),
                legend.spacing = unit(0.2, "cm"),
                legend.title = element_text(face="italic"),
                plot.margin=unit(c(10,5,5,5),"mm"),
                strip.background=element_rect(colour="#f0f0f0",fill="#f0f0f0"),
                strip.text = element_text(face="bold")
                ))
    
}
## Aux functions
f.rib <- function(X, dpad = 5)
    geom_ribbon(aes(ymin= y - X*dpad, ymax = y + X*dpad), fill = "blue", alpha = 0.01)

make.q <- function(from, to, cfs, normal=TRUE, sig, size=100){
    if(normal)
        df <- data.frame(x = seq(from, to, length =size)) %>%
            mutate(y = cfs[1]+cfs[2]*x,
                   low.1 =  qnorm(0.05, size, mean = y, sd =sig),
                   upp.1 =  qnorm(0.95, size, mean = y, sd =sig),
                   low.2 =  qnorm(0.25, size, mean = y, sd =sig),
                   upp.2 =  qnorm(0.75, size, mean = y, sd =sig))
    else
        df <- data.frame(x = seq(from, to, length =size)) %>%
            mutate(y = cfs[1]+cfs[2]*x,
                   low.1 =  qpois(0.05, size, lambda = y),
                   upp.1 =  qpois(0.95, size, lambda = y),
                   low.2 =  qpois(0.25, size, lambda = y),
                   upp.2 =  qpois(0.75, size, lambda = y))
} 

plot.q <- function(df2, df1){
    ggplot(df2, aes(x, y)) +
        geom_line(col="navy") +
        geom_ribbon(aes(ymin = low.1, ymax = upp.1), fill="blue", alpha =0.25) +
        geom_ribbon(aes(ymin = low.2, ymax = upp.2), fill="blue", alpha =0.25) +
        geom_point(data = df1, color ="red", size=1.25, stroke=1.25)  +
        theme_Publication(18)
    }
```

# Recap

* Statistical models describe the distribution of probabilities of
  random variables.

--

* This is done by turning parameters of probability distributions into
  functions of predictor variables.

--

* In the simple linear regression model, a response $y_i$ is described
  as a Gaussian random variable, with a location parameter $\mu$ as a
  linear function of a variables $x_i$:
  
$$
\begin{align}
y_i &\sim \mathcal{N}(\mu_i, \sigma) \\
\mu_i & = \beta_{0} + \beta_1 x_i
\end{align}
$$

---

# Recap

* The least squares method (LS) is a way to fit the linear
  regression model, by minimizing the sum of the squared residuals.

--

* LS estimators of the parameters of the linear predictor for a simple
  linear regression are:

$$
\begin{align}
\\
\hat{\beta_1}&=\frac{\sum(x_i-\bar{x})(y_i-\bar{y})}{\sum (x_i - \bar{x})^2 }\\[2em]
\hat{\beta_0}&=\bar{y}-\hat{\beta_1}\bar{x}
\end{align}
$$

---

# Today we'll see ...

* that LS is a rather good idea to fit Gaussian models, as it is a
particular case of the **Maximum Likelihood Method** (ML)

--

* that ML provides a general criterium to find the parameter values that
  are best supported by data, for any statistical model.

--

* how ML estimates (MLEs) can be calculated (or how we can fit models
  to data with the ML method, which is the same).

---

# Our Good Ol'friend, the linear regression

.pull-left[

$$
\begin{align}
y_i &\sim \mathcal{N}(\mu_i, \sigma) \\
\mu_i & = \beta_0 + \beta_i x_i \\
\sigma & = C \\
\end{align}
$$

]


.pull-right[
.center[
```{r }
set.seed(42)
a0 <- 10
a1 <- 30
df1 <- data.frame(x=runif(99))
df1$y <- rnorm(nrow(df1), mean = a0 + a1*df1$x, sd = 5)
df1 <- rbind(df1, c(x=0.5, y=32))
lm1 <- lm(y~x, data=df1)
cf1 <- coef(lm1)
df2 <- data.frame(x = seq(0,1, by=0.01))
df2$y <- predict(lm1, df2)

ggplot(df2, aes(x, y)) +
    geom_line(color="navy") +
    lapply(seq(0.1,3.5, by=0.05), f.rib) +
    geom_point(data = df1, color ="red", size=1.25, stroke=1.25)  +
    theme_Publication(18) +
    xlab("X") +
    ylab("Y") 
```
]
]

---

# Models assign probabilities to observations

.pull-left[

$$
\begin{align}
y_i &\sim \mathcal{N}(\mu_i, \sigma) \\
\mu_i & = \beta_0 + \beta_i x_i \\
\sigma & = C \\
\end{align}
$$

### Example: 

What's the Normal density for $\{x=0.5, y=32\}$?

]

.pull-right[
.center[
```{r }
df2 <- make.q(0, 1, cfs = c(10,30), sig = 5)

p1 <-
    plot.q(df2,df1) +
    ggtitle(bquote(beta[0]==10~beta[1]==30)) 

p1b <-
    p1 +
    theme(plot.margin=margin(b=1, l=1, r = 5, unit="lines")) +
    coord_cartesian(xlim = c(0, 1.2), clip = "off") +
    annotate("text", x = 1.12, y = 40, label= "50%", color="blue", size=5) +
    annotate("text", x = 1.32, y = 40, label= "95%", color="blue", alpha =0.75, size=5) +
    annotate("segment", x = 1.05, xend = 1.05, y = df2[nrow(df2),]$low.2, yend = df2[nrow(df2),]$upp.2,
             colour = "blue", linewidth = 1.25) +
    annotate("segment", x = 1.25, xend = 1.25, y = df2[nrow(df2),]$low.1, yend = df2[nrow(df2),]$upp.1,
             colour = "blue", alpha = 0.75, linewidth = 1.25) +
    geom_segment(aes(x = 0.5, xend = 0.5, y = 0, yend = 32), lty = 2, col ="navy") +
    geom_segment(aes(x = 0, xend = 0.5, y = 32, yend = 32), lty = 2, col ="navy") +
    geom_point(data=data.frame(x=0.5, y=32), col="red", size=2.5, stroke =2)

p1b

```
]
]

---

# Models assign probabilities to observations

.pull-left[

$$
\begin{align}
y_i &\sim \mathcal{N}(\mu_i, \sigma) \\
\mu_i & = \beta_0 + \beta_i x_i \\
\end{align}
$$

### Example: 
Normal density for $\{x=0.5, y=32\}$:

```{r , echo=TRUE}
(mu <- 10 + 30*0.5)
dnorm(32, mean = mu, sd = 5)
```

]

.pull-right[
.center[

```{r }

p1b +
    geom_segment(aes(x = 0.5, xend = 0, y = mu, yend = mu), lty = 2, col ="navy")
    

```
]
]

---


## Statistical models are hypotheses ... 

.center[

.pull-left[
```{r }
p1c <-
    p1 +
    ggtitle(bquote(H1:~ beta[0]==10~beta[1]==30))
p1c
```
]

.pull-right[
```{r }
new.cfs <- c(mean(df1$y) - 15*mean(df1$x), 15)
p2 <-
    make.q(0, 1, cfs = new.cfs, sig = summary(lm1)$sigma) %>%
    plot.q(df1) +
    ggtitle( bquote(H2: ~ beta[0]==.(round(new.cfs[1],1))~beta[1]==.(round(new.cfs[2],1))))
p2
```
]
]

---

## ... about the process that generated the data

.center[

.pull-left[
```{r }
p1c
```
]]

.center[
.pull-right[
```{r  }
make.q(0, 1, cfs = c(10,30), sig = 5 * seq(0.5,1.5,length=100)) %>%
    plot.q(df1) +
    ggtitle(bquote(H3: ~ beta[0]==10~beta[1]==30))
```
]]

---

# Likelihood: a measure of support

### The likelihood law (informal statement)

.bg-washed-blue.b--navy.ba.bw2.br3.shadow-5.ph4.mt1[
.center[**IF**]

* There are many hypotheses or models to describe a given data set, 
* and each model assigns different probabilities to data
]

--

.center[
.bg-washed-blue.b--navy.ba.bw2.br3.shadow-5.ph4.mt1[
**THEN**

The model that assigns the larger probability to data is the best
supported by the data, or the more plausible model.  ]
]

---

# Likelihood law: a simple example

.pull-left[
**Two hypotheses**

* $H_A$: the balls in the urn are all red
* $H_B$: half of the balls are red and half are white

**Data collecting**

Draw balls at random from the urn, with replacement
]

.center[
.pull-right[
```{r, echo=FALSE}
knitr::include_graphics("figs/urn2.png")
```
]]

---

# Likelihood law: a simple example

### A single drawn, and the ball is red

$$
\begin{align}
\\
P(\text{red} \mid H_A) & = 1\\
P(\text{red} \mid H_B) & = 0.5
\\
\end{align}
$$

$H_A$ is $\frac{1}{0.5}\; = \; 2$ times more plausible (or better
supported by the observation) than $H_B$.

---


# Likelihood law: a simple example

### Two draws, both of red balls

$$
\begin{align}
\\
P(\{\text{red, red}\} \mid H_A) & = 1 \times 1 = 1\\
P(\{\text{red, red}\} \mid H_B) & = 0.5 \times 0.5 = 0.25
\\
\end{align}
$$

$H_A$ is $\frac{1}{0.25}\; = \; 4$ times more plausible (or better
supported by the observations) than $H_B$. <sup>1</sup> 

.footnote[[1] under the assumption that the data are independent observations]

---

# The likelihood function

Any function proportional to the product of probabilities assigned by
a statistical model to each observation in data:


.bg-washed-blue.b--navy.ba.bw2.br3.shadow-5.ph4.mt1[
$$ \mathcal{L} \propto P(y_1 \mid H) \times P(y_2 \mid H) \times \ldots P(y_i \mid H)$$
]


---

# The likelihood function

Any function proportional to the product of probabilities assigned by
a statistical model to each observation in data:


.bg-washed-blue.b--navy.ba.bw2.br3.shadow-5.ph4.mt1[
$$ \mathcal{L} \propto \prod^N P(y_i \mid H)$$
]

---

# The log-likelihood function

The logarithm of the likelihood function, that is, any function
proportional to the **sum of the probabilities** assigned by a model
to each observation in data:

.bg-washed-blue.b--navy.ba.bw2.br3.shadow-5.ph4.mt1[
$$ \textbf{L} \propto \sum^N \ln P(y_i \mid H)$$
]


---

# Important remarks

**The likelihood is a function of the model parameters:** likelihoods
are conditional to a given data set. That is, data is taken as an
invariant, while the model parameters can change.

--

**Likelihood functions are not probability distribution functions**,
though you need probabilities distributions to calculate likelihoods.

--

**Notation** 

We will use to notation $L_y(\theta)$ to express:

* a likelihood function conditional to a given data set $y = {y_1, \ldots, y_n}$ ,

* for a model that has a set of parameters $\theta = {\theta_1, \ldots, \theta_i}$.

---

# Likelihood function of simple linear regression

The probability density function of the Gaussian for a given observation $y_i$ is

$$\mathcal{N}(y_i) \, = \, (2 \pi \sigma^2)^{-\frac{1}{2}} \, e^{-\frac{1}{2\sigma^2}  (y_i - \mu_i)^2}$$

--

In the linear regression, $\mu_i = \beta_0 + \beta_1 x_i$. Substituting we have:

$$\mathcal{N}(y_i) \, = \, (2 \pi \sigma^2)^{-\frac{1}{2}} \, e^{-\frac{1}{2\sigma^2}  (y_i - (\beta_0 + \beta_1 x_i))^2}$$

--

Thus, the parameter set of the simple linear regression is 
$\theta =\{ \beta_0, \beta_1, \sigma \}$ 
and the likelihood function is the
product of the density function defined above over all values of
$y_i$:

$$\mathcal{L}_y (\theta) \, = \, \prod_{i=1}^n \, (2 \pi \sigma^2)^{-\frac{1}{2}} \, e^{-\frac{1}{2\sigma^2}  (y_i - (\beta_0 + \beta_1 x_i))^2}$$


---

# Example of likelihood for one parameter

.pull-left[
.center[
```{r }
    make.q(0, 1, cfs = c(5,30), sig = 5) %>%
    plot.q(df1) +
        ggtitle(bquote(beta[0]==5~beta[1]==30)) +
        ylim(c(-5, 50))
```
]]

.pull-right[
.center[

```{r }
ll1 <- function(b0)
    sum(dnorm(df1$y, mean = b0 + 30*df1$x, sd = 5, log=TRUE))
b0 <- seq(-0.5,20, by=0.5)
par(las=1, mar=c(6,7,4,2)+0.1,
    mgp=c(5,1,0),
    cex.lab=2, cex.axis=1.8, cex.main = 2.25)
plot(b0, sapply(b0, ll1), type = "l", lwd =1.5, col="blue",
     xlab = expression(beta[0]),
     ylab = expression(L[y](beta[0])),
     main = expression(paste("Log-lik for ",beta[0]," , ",beta[1]==30,",",sigma==5)))
points(5, ll1(5), pch =19, cex =2, col="red")
```
]
]

---

# Example of likelihood for one parameter

.pull-left[
.center[
```{r }
p1 + ylim(c(-5, 50))
```
]]

.pull-right[
.center[

```{r }
par(las=1, mar=c(6,7,4,2)+0.1,
    mgp=c(5,1,0),
    cex.lab=2, cex.axis=1.8, cex.main = 2.25)
plot(b0, sapply(b0, ll1), type = "l", lwd =1.5, col="blue",
     xlab = expression(beta[0]),
     ylab = expression(L[y](beta[0])),
     main = expression(paste("Log-lik for ",beta[0]," , ",beta[1]==30,",",sigma==5)))
points(10, ll1(10), pch =19, cex =2, col="red")
```
]
]

---

# The method of maximum likelihood

  When you choose one or more statistical models to describe data,
  you do not know the values of the model parameters.

.bg-washed-blue.b--navy.ba.bw2.br3.shadow-5.ph4.mt1[
  **Fitting by the ML method**: 
  
  To find the values of the parameters that are best supported by
  data, that is, the values of the parameters that maximize the
  likelihood function:
  
  $$\widehat{\theta} =  \underset{\theta}{\mathrm{argmax}}[L_y(\theta)]$$
]


---

# Simple linear regression: MLEs deduction 

### Log-likelihood function of the model:

$$
\begin{align}
& \textbf{L}_y(\mu,\beta_0,\beta_1) = \\[1em] 
&\ln [ \mathcal{L}_y (\mu,\beta_0,\beta_1) ] = \\[1em]
& \ln \left[ \prod_{i=1}^n \, (2 \pi \sigma^2)^{-\frac{1}{2}} \, e^{-\frac{1}{2\sigma^2}  (y_i - (\beta_0 + \beta_1 x_i))^2} \right] = \\[1em]
& \frac{n}{2} \ln(2 \pi) \, + \, \frac{n}{2} \ln(2 \sigma^2) \, + \, \frac{1}{2 \sigma^2}\, \sum_{i=1}^n (y_i - \beta_0 -\beta_i x_i)^2
\end{align}
$$


---
# Simple linear regression: MLEs deduction 

### Take derivatives and set equal to zero

$$
\begin{align}
\frac{\partial \textbf{L}_y(\beta_0,\beta_1,\sigma^2)}{\partial \beta_0} =  0 & \implies \sum y_i \, = \, n \beta_0 + \beta_1 \sum x_i \\[1em]
\frac{\partial \textbf{L}_y(\beta_0,\beta_1,\sigma^2)}{\partial \beta_1} = 0 & \implies \sum y_i x_i \, = \, \beta_0 \sum x_i + \beta_1 \sum x_i^2 \\[1em]
\frac{\partial \textbf{L}_y(\beta_0,\beta_1,\sigma^2)}{\partial \sigma^2} =  0 & \implies n - \frac{1}{\sigma^2} \sum (y_i - \beta_0 - \beta_1)^2 \, = \, 0
\end{align}
$$

---

# LS method is the maximum likelihood solution for Gaussian models!

### Solving the system with the first two equations:

$$
\begin{cases}
\sum y_i   =  n \beta_0 + \beta_1 \sum x_i \\[1em] 
\sum y_i x_i  =  \beta_0 \sum x_i + \beta_1 \sum x_i^2
\end{cases}
$$

### We get:

$$
\hat{\beta_1}\, =\, \frac{\sum(x_i-\bar{x})(y_i-\bar{y})}{\sum (x_i - \bar{x})^2 } \; \;  , \; \;  
\hat{\beta_0}\, = \, \bar{y}-\hat{\beta_1}\bar{x} 
$$


---

# Some other analytical MLEs

To fit distributions to data (very simple models, no predictors):

* Poisson: $\; \hat{\lambda} \, = \, \frac{\sum{y_i}}{n}$ 

* Normal: $\; \hat{\mu} \, = \, \frac{\sum{y_i}}{n}$ , $\; \hat{\sigma} \, = \, \frac{\sqrt{(y_i - \bar{x})^2}}{n}$

* Binomial: $\; \hat{p} \, = \, \frac{y}{N}$

* Exponential: $\; \hat{\lambda} \, = \, \frac{n}{\sum{y_i}}$

* Geometric: $\; \hat{p} \, = \, \frac{n}{n + \sum y_i}$

---

# Example: Poisson ML fitting


.pull-left[
.center[
![](./figs/euterpe_edulis.jpeg)
]

* 568 plots in the Atlantic Forest
* A total of 4242 palm trees recorded.
* To fit a Poisson, we need just the sample mean:

$$ \hat{\lambda}  =  \frac{4242~\text{trees}}{568~\text{plots}}   =  7.468/ \text{plot} $$

]

.center[
.pull-right[
```{r euterpe-data}
euterpe = read.csv("euterpe.csv",as.is=T)
euterpe$adulta.viva[is.na(euterpe$adulta.viva)] <- 0
x <- 0: max(euterpe$adulta.viva, na.rm=TRUE)
yobs <- as.numeric(table(factor(euterpe$adulta.viva, levels = x)))
yobsp <- yobs/sum(yobs)
eut.fitp <- fitdistr(euterpe$adulta.viva, "Poisson")
eut.fitnb <- fitdistr(euterpe$adulta.viva, "negative binomial")
eut.p <- dpois(x, lambda = mean(euterpe$adulta.viva))
eut.nb.cf <- coef(eut.fitnb)
eut.nb <- dnbinom(x, size = eut.nb.cf[1], mu = eut.nb.cf[2])
## pdf("../figuras/euterpe%1d.pdf",height=8,width=11, bg="lightblue", fg="black", onefile=FALSE)
par(cex.axis = 2, cex.lab = 2.5, cex.main=3, mar = c(6,7,4,2), mgp=c(4.8,1,0), bty = "l", las=1)
## SÃ³ obs
plot(x, yobsp, type = "s", lwd =3, xlab = "N trees/plot", ylab = "Proportion of plots")
```
]]


---

# Example: Poisson ML fitting

.center[
```{r}
## Obs e Poisson
par(cex.axis = 2, cex.lab = 2.5, cex.main=3, mar = c(6,7,4,2), mgp=c(4.8,1,0), bty = "l", las=1)
plot(x, yobsp, type = "s", lwd =3, xlab = "N trees/plot", ylab = "Proportion of plots")
lines(x, eut.p, col = "red", lwd =3)
legend("topright", c("Obs", expression(P(lambda==7.468))), col = c("black", "red"),
       lty =1, bty = "n", cex = 2, lwd = 2)
```
]

---

# Poisson x Negative Binomial

.pull-left[

**Which model is best supported by this data?**

* Log-Likelihoods differences: 

$$L_{NB} - L_{Poisson} =  1924$$

* That is, a likelihood ratio of $e^{1924}$

* The negative binomial fit is overly best supported by the data, **compared** to the Poisson fit.
]

.pull-right[
.center[
```{r }
## Obs, Poisson, BN
par(cex.axis = 2, cex.lab = 2.5, cex.main=3, mar = c(6,7,4,2), mgp=c(4.8,1,0), bty = "l", las=1)
plot(x, yobsp, type = "s", lwd =3, xlab = "N trees/plot", ylab = "Proportion of plots")
lines(x, eut.p, col = "red", lwd =3)
lines(x, eut.nb, col = "darkblue", lwd =3)
legend("topright", c("Obs", expression(P(lambda==7.468)), expression(paste(NB(mu==7.468,nu==0.642)))),
       col = c("black", "red", "darkblue"),
       lty =1, bty = "n", cex = 1.75, lwd = 2)
```
]]


---

# Finding MLEs numerically

.pull-left[

* In most cases analytical solutions for MLEs are not available. 

* In part because even relatively simple models have more than 3
parameters and thus, **complex, multidimensional likelihood
surfaces**.

* We have to rely on numerical methods, usually
using computers (optimization routines).
] 

.pul-right[
.center[
[![](./figs/optimization.gif)](https://towardsdatascience.com/a-visual-explanation-of-gradient-descent-methods-momentum-adagrad-rmsprop-adam-f898b102325c)
]]

???

In most cases a analytical solution is not available. In part because even simple models have more than 3 parameters, linked by complicated functions. We have to rely on numerical methods, usually using computers

Figure of LL surface of a Gaussian and of the contour map with optimization route
https://towardsdatascience.com/a-visual-explanation-of-gradient-descent-methods-momentum-adagrad-rmsprop-adam-f898b102325c

---

# Optimization is tricky

.pull-left[
.center[
[![](./figs/one_thing.png)](https://www.youtube.com/watch?v=_U-7L1tmBAo)
]
]

.pull-right[
.center[
[![](./figs/two_optima.gif)](https://towardsdatascience.com/a-visual-explanation-of-gradient-descent-methods-momentum-adagrad-rmsp)
]]


---


# Look for ML fitting in good libraries

* There are libraries for ML fitting of a huge variety of models.

* To start using a class of statistical models, look carefully for
  stable and affordable software. Advice of more experienced users is the 
  safest path.

* Good libraries embody the expertise of collaborating statisticians,
  computer scientists and users, *e.g.*:

  * Linear regression LS: `stats::lm()`
  * Non-linear LS: `stats::nls`
  * Generalized linear models: `stats::glm()`
  * Mixed effect Gaussian models : `lme4::lmer`
  * Mixed effects glm's: `lme4::glmer`
  * ...


---

# Properties of MLEs

### as $n \rightarrow \infty$ :

* **Consistency**: converges to the value of the parameter to be estimated, in probability.

* **Efficiency**: has the lowest possible mean-square error, for a consistent estimator. 

* **Normality**: the sample distribution of MLE's asymptotically tends to normality.<sup>2</sup> 

### In general:

* **Functional equivariance**: a monotonic function of a MLE is also a MLE.


.footnote[[2] A generalization of Central Limit Theorem, follows from the efficiency property]

---

# Inference about parameters 

```{r }
Birds <- read.csv("DatasetS1-IndivBirdParasiteLoad.csv")
Birds.m <- 
    Birds %>%
    group_by(BirdSpecies) %>%
    summarise(m.Mh = mean(Mh.g.), m.Wp = mean(Wp.tot),
              lm.Mh = log10(m.Mh), lm.Wp = log10(m.Wp))


## starting values
Bird.m0 <- lm(lm.Wp~lm.Mh, data=Birds.m)
cfs <- list(coef(Bird.m0)[1], coef(Bird.m0)[2], summary(Bird.m0)$sigma)
names(cfs) <- c("beta0", "beta1", "sigma")

Birds.m1 <- mle2(lm.Wp ~ dnorm(beta0 + beta1* lm.Mh, sd =sigma),
                 start = cfs,
                 data = Birds.m,
                 parameters=list(beta0~1, beta1~1, sigma ~1))
Birds.p <- profile(Birds.m1)
```

.pull-left[

### Log-likelihood profiles

* A plot of the minimum of the **negative log-likelihood** value as a
function of a single parameter;

* Y-axis standardized such as its minimum is zero.

* **Canonical rule**: all parameter values within 2 units of the relative
  negative log-likelihood are equally plausible.

]

.pull-right[
.center[
```{r }
par(las=1, mar=c(6,7,4,2)+0.1,
    mgp=c(5,1,0),
    cex.lab=2, cex.axis=1.8, cex.main = 2.25)
plotprofmle(Birds.p, which =3, lwd =2) 	
```
]]

---

# Danger Will Robinson!

.pull-left[
.center[
[![](./figs/one_thing.png)](https://www.youtube.com/watch?v=_U-7L1tmBAo)
]
]

.pull-right[
.center[
![](figs/amostra100.png)
]]

---

# Example: scaling of parasite loads

.pull-left[
* The biomass of bird ectoparasites is proportional to the $2/3$
power of host body mass.
* Data: Average body biomass and average ectoparasites mass per bird for 42
   bird species.
* Simple linear regression of log of average parasite biomass as the response, and log of average body mass as the predictor variable.
 
.footnote[Hechinger et al. Proc. R. Soc, 2019]
 ]


.pull-right[
.center[
```{r }
par(las=1, mar=c(6,7,4,2)+0.1,
    mgp=c(5,1,0),
    cex.lab=2, cex.axis=1.8, cex.main = 2.25)
plot(lm.Wp ~ lm.Mh, data = Birds.m,
     xlab = "log Average body mass (g)",
     ylab = "log Average parasite mass (g)",
     pch=19, col="blue")
```
]]

---

# Plausibility intervals

.pull-left[
.center[

```{r }
par(las=1, mar=c(6,7,4,2)+0.1,
    mgp=c(5,1,0),
    cex.lab=2, cex.axis=1.8, cex.main = 2.25)
plotprofmle(Birds.p, which =2, lwd =2)
arrows(x0 = 2/3, y0= -0.2, x1=2/3, y=.5, code =1, lwd = 2)
```
]
]

.pull-right[
.center[
```{r }
par(las=1, mar=c(6,7,4,2)+0.1,
    mgp=c(5,1,0),
    cex.lab=2, cex.axis=1.8, cex.main = 2.25)
plot(lm.Wp ~ lm.Mh, data = Birds.m,
     xlab = "log Average body mass (g)",
     ylab = "log Average parasite mass (g)",
     pch=19, col="blue")
abline(Bird.m0, lwd= 1.5)
```
]]

---

## Take-home messages

+ Any statistical model hypothesis on how data has been generated. As
  such, these hypotheses assigns probabilities to data.

--

+ Likelihood law: stick with the model that assigns the larger probability to your data

--

+ The (log) likelihood expresses the support of the data for a given
  model, **compared** to other models fitted to the same data set.

--

+ We can fit statistical models by finding the values of the
  parameters that maximize the model likelihood.

--

+ Maximum likelihood estimates have many nice statistical properties.

---

# Further reading

+   Likelihood and all that. chapter 6 *in* Bolker, B.M. 2008
    Ecological Models and Data in R. Princeton: Princeton University
    Press.

+   Hobbs, N.T. & Hilborn, R. 2006. Alternatives to statistical
    hypothesis testing in ecology: A guide to
    self-teaching. Ecological Applications: 16(1): 5-19. 

+   The  Concept of Likelihood , chapter 2 *in*: Edwards, A.W.F., 1992
    Likelihood. Baltimore: John Hopkins University Press.  

